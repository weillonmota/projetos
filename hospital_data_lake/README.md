# üè• Projeto: Data Lake Hospital Universit√°rio Onofre Lopes

## üéØ Vis√£o Geral do Projeto

Este projeto visa dar um suporte ao **Hospital Universit√°rio Onofre Lopes (HUOL)** em Natal/RN com objetivo de estudar a concorr√™ncia das interna√ß√µes e analisar a reputa√ß√£o do Hospital no Instagram.

Vamos cruzar dois tipos de dados: o **estruturado** (dados de interna√ß√µes hist√≥ricas do SUS) com o **n√£o estruturado** coment√°rios postados na p√°gina do Hospital no Instagram para extrair insights valiosos, usando **Machine Learning** e **an√°lise estat√≠stica**, para apoiar a decis√£o estrat√©gica de expans√£o do servi√ßo de interna√ß√£o.

----------

## üõ†Ô∏è Fase 1: Data Lake, Infra e Ingest√£o Bruta (Bronze)

A primeira camada do Data Lake √© a **Bronze** que prepara o ambiente para receber os dados brutos.

### 1. Configura√ß√£o da Infraestrutura (AWS S3)

-   **Onde Guardamos a Bagun√ßa:** Escolhemos o **Amazon S3 (Simple Storage Service)**, o _storage_ de objetos padr√£o de mercado, utilizando o **Free Tier** da AWS para garantir escalabilidade e durabilidade a custo zero na largada.
    
-   **Estrutura:** Criamos o _bucket_ (`datalake-sad-huol-weillon`) e organizamos a arquitetura do Data Lake em tr√™s camadas, seguindo as melhores pr√°ticas:
    
    -   `/bronze`: **Dados Brutos** (raw data).
        
    -   `/silver`: **Dados Limpos/Enriquecidos** (prontos para an√°lise).
        
    -   `/gold`: **Data Warehouse (DW)** modelado (tabela Fato e Dimens√µes).
        

### 2. Acesso Seguro e M√≠nimo (IAM)

A seguran√ßa √© _prioridade zero_! Bloqueamos o acesso p√∫blico ao _bucket_ e criamos um mecanismo seguro para o nosso c√≥digo interagir com o S3.

-   **Princ√≠pio:** Aplicamos o **Princ√≠pio do Menor Privil√©gio (PoLP)**.
    
-   **Usu√°rio Program√°tico:** Criamos o usu√°rio IAM (`user-projeto-sad-huol`) para uso em c√≥digo local.
    
-   **Pol√≠tica Customizada:** Anexamos a pol√≠tica **`Policy-SAD-HUOL-S3-Acesso-M√≠nimo`**. Esta pol√≠tica restringe o acesso _apenas_ ao nosso _bucket_ espec√≠fico e permite **somente** as a√ß√µes necess√°rias (`ListBucket`, `GetObject`, `PutObject`).
    



Abaixo, o _screenshot_ da pol√≠tica customizada criada no console IAM, garantindo a restri√ß√£o de acesso ao S3.

![Pol√≠ticas IAM](./evidencias/001.jpg)

### 3. Coleta e Ingest√£o de Dados Estruturados (SUS)

Os dados estruturados do HUOL foram coletados no portal do Governo Federal, exigindo autentica√ß√£o `gov.br` para _download_.

-   **Fonte:** Base de **Interna√ß√µes Hospitalares** do HUOL/UFRN.
    
-   **Per√≠odo:** Janeiro de 2024 a Setembro de 2025.
    
-   **Ingest√£o Bronze:** Os 7 arquivos CSV brutos foram carregados na camada de entrada do nosso Data Lake.
    

#### **[Visualiza√ß√£o 2: Evid√™ncia da Camada Bronze]**

Aqui est√° o _screenshot_ do _bucket_ S3, demonstrando a presen√ßa dos 7 arquivos CSV brutos de interna√ß√µes na pasta `/bronze`.

![bucket S3](/evidencias/002.jpg)


### 4. Coleta de Dados N√£o Estruturados (Web Scraping com Selenium)

Para complementar a camada bronze √© preciso coletar os dados do Instagram do Hospital. Inicialmente tentamos o utilizar o API da pr√≥pria META, por√©m sem √™xito, devido a complexidade da autentica√ß√£o e restri√ß√µes de permiss√£o, foi necessario mudar de estrat√©gia para uma solu√ß√£o robusta de **Web Scraping** para a coleta dos coment√°rios na p√°gina do Hospital. Ap√≥s coletar os dados brutos do instagram conseguimos subir esses dados para o S3 na Amazon finalizando assim nossa camada bronze.

-   **Vantagem:** O **Selenium** simula um usu√°rio logado no navegador Firefox, garantindo a coleta de todos os coment√°rios p√∫blicos e superando a complexidade das permiss√µes da API.
    
-   **Seguran√ßa:** Implementamos a leitura segura de credenciais via arquivo **`.env`**, garantindo que a senha de acesso (necess√°ria para o login do Selenium) n√£o seja exposta no c√≥digo.
    

----------

Segue abaixo o codigo do scraper que foi utilizado para coletar os dados do instagram do hospital.

## üíª C√≥digo do Scraper (coleta_instagram.py)

O script mapeia e extrai os dados do perfil do HUOL e salva as informa√ß√µes no formato CSV (que √© o formato final que ser√° carregado no S3).

_Pr√©-requisitos:_ `pip install selenium webdriver-manager python-dotenv`

Python

```
import csv
import time
import os
from typing import List, Dict, Any, Set
from selenium import webdriver
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.firefox.options import Options
from webdriver_manager.firefox import GeckoDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.remote.webdriver import WebDriver
from dotenv import load_dotenv

# --- CARREGAR VARI√ÅVEIS DE AMBIENTE (.env) ---
# Garante que as credenciais sens√≠veis n√£o fiquem expostas no c√≥digo.
load_dotenv() 

# --- CONFIGURA√á√ïES DO SCRAPER (Lidas do .env) ---
INSTAGRAM_USERNAME = os.getenv("INSTAGRAM_USERNAME") 
INSTAGRAM_PASSWORD = os.getenv("INSTAGRAM_PASSWORD") 
TARGET_PROFILE = "https://www.instagram.com/huol_ufrn/"
CSV_FILENAME = "comentarios_huol_completo_223posts.csv"

# --- CLASSE PRINCIPAL ---
class InstagramCommentScraper:
    # O construtor recebe as credenciais (que vieram do .env)
    def __init__(self, username: str, password: str) -> None:
        self.username = username
        self.password = password
        self.driver = self._setup_driver()
        self.wait = WebDriverWait(self.driver, 20) 

    def _setup_driver(self) -> WebDriver:
        # Configura√ß√£o do Selenium para usar o Firefox
        firefox_options = Options()
        # firefox_options.add_argument("--headless") # Descomentar para rodar em modo invis√≠vel
        service = Service(GeckoDriverManager().install()) # Garante o driver mais recente
        return webdriver.Firefox(service=service, options=firefox_options)

    def login(self) -> None:
        """Realiza o login automatizado no Instagram."""
        print("üîê Fazendo Login...")
        self.driver.get("https://www.instagram.com/accounts/login/")
        time.sleep(5)
        try:
            try: self.driver.find_element(By.XPATH, "//button[text()='Allow all cookies' or text()='Permitir todos os cookies']").click()
            except: pass
            
            # Preenche nome de usu√°rio e senha, lidos do .env
            self.wait.until(EC.element_to_be_clickable((By.NAME, "username"))).send_keys(self.username)
            time.sleep(1)
            self.driver.find_element(By.NAME, "password").send_keys(self.password)
            time.sleep(1)
            
            # Clica no bot√£o de login
            self.driver.find_element(By.CSS_SELECTOR, "button[type='submit']").click()
            
            # Espera carregar a p√°gina inicial para confirmar o sucesso
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "svg[aria-label='Home'], svg[aria-label='P√°gina inicial']")))
            print("‚úÖ Login realizado com sucesso.")
            time.sleep(5)
        except Exception as e:
            print(f"‚ùå Falha no login. Verifique as credenciais no .env. Erro: {e}")
            self.driver.quit(); exit()

    def get_all_post_links(self, profile_url: str, target_count: int = 300) -> List[str]:
        """Rola a p√°gina do perfil para mapear URLs de posts."""
        print(f"üîç Mapeando posts do perfil: {profile_url}")
        self.driver.get(profile_url)
        time.sleep(5)
        
        post_links: Set[str] = set()
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        tentativas_sem_novos = 0
        
        print("   üîÑ Rolando perfil para carregar posts antigos...")
        
        # Loop de rolagem para carregar dinamicamente mais posts
        while len(post_links) < target_count:
            # Coleta todos os links de posts vis√≠veis (URLs com '/p/')
            anchors = self.driver.find_elements(By.XPATH, "//a[contains(@href, '/p/')]")
            for anchor in anchors:
                href = anchor.get_attribute("href")
                if href: post_links.add(href)
            
            print(f"      -> {len(post_links)} posts encontrados...")
            
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3) 
            
            # L√≥gica para sair do loop se chegar ao fim da p√°gina
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                tentativas_sem_novos += 1
                if tentativas_sem_novos >= 3: 
                    print("   ‚úÖ Fim da p√°gina de perfil (N√£o h√° mais posts para carregar).")
                    break
            else:
                tentativas_sem_novos = 0
                last_height = new_height
                
        lista_final = list(post_links)
        print(f"üìå Mapeamento conclu√≠do: {len(lista_final)} posts prontos para extra√ß√£o.")
        return lista_final

    def extract_comments_from_post(self, post_url: str) -> List[Dict[str, Any]]:
        """Acessa o post, carrega e extrai dados dos coment√°rios."""
        print(f"\nüì• Acessando: {post_url}")
        self.driver.get(post_url)
        time.sleep(5) 
        
        extracted_data: List[Dict[str, Any]] = []
        seen_comments: Set[tuple] = set()
        
        try:
            post_id = post_url.split("/p/")[1].split("/")[0]
        except: post_id = "post_unk"

        try:
            li_elements = self.driver.find_elements(By.XPATH, "//ul//li")
            count_inicial = len(li_elements)
            
            # --- L√ìGICA H√çBRIDA (Interven√ß√£o Manual para posts grandes) ---
            if count_inicial > 17: 
                print("   üö® POST GRANDE! Necess√°rio carregar manualmente.")
                print("   üëâ Role a barra de coment√°rios no Firefox at√© o fim.")
                input("   üëâ Pressione ENTER aqui quando terminar de carregar TUDO...") # Pausa a execu√ß√£o
                print("   ü§ñ Ok! Capturando dados...")
                
                # Atualiza a lista ap√≥s rolagem manual do usu√°rio
                li_elements = self.driver.find_elements(By.XPATH, "//ul//li")
            
            # --- EXTRA√á√ÉO ---
            for li in li_elements:
                try:
                    full_text = li.text.strip()
                    lines = full_text.split('\n')
                    
                    autor = "Desconhecido"
                    if lines: autor = lines[0].strip()
                    
                    # Ignora a legenda do post e elementos de interface
                    is_legenda = False
                    try:
                        if li.find_element(By.TAG_NAME, "h1"): is_legenda = True
                    except: pass
                    if is_legenda: continue 

                    texto_parts = []
                    # Blacklist de termos de interface do usu√°rio
                    blacklist = ["Responder", "Reply", "Enviar", "Editado", "Ver insights", "Ocultar", autor]
                    
                    for line in lines:
                        l = line.strip()
                        if l in blacklist: continue 
                        if "curtida" in l or "like" in l: continue 
                        texto_parts.append(l)
                        
                    texto_final = " ".join(texto_parts).strip()

                    # Coleta Metadados
                    data_post = "N/A"
                    try:
                        time_tag = li.find_element(By.TAG_NAME, "time")
                        data_post = time_tag.get_attribute("datetime")
                    except: pass

                    curtidas = "0"
                    if "curtida" in full_text:
                         for l in lines:
                             if "curtida" in l: curtidas = l
                             
                    signature = (autor, texto_final)
                    # Adiciona se o texto for v√°lido e n√£o for duplicata
                    if texto_final and signature not in seen_comments:
                        seen_comments.add(signature)
                        extracted_data.append({
                            "ID Post": post_id, 
                            "Autor": autor, 
                            "Data": data_post,
                            "Texto": texto_final,
                            "Curtidas": curtidas
                        })

                except Exception:
                    continue
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao processar post: {e}")
            
        return extracted_data

    def close(self): self.driver.quit()

def main():
    # Verifica se as credenciais foram carregadas
    if not INSTAGRAM_USERNAME or not INSTAGRAM_PASSWORD:
        print("‚ùå ERRO DE CONFIGURA√á√ÉO: O INSTAGRAM_USERNAME ou INSTAGRAM_PASSWORD n√£o foi carregado do arquivo .env.")
        return

    scraper = InstagramCommentScraper(INSTAGRAM_USERNAME, INSTAGRAM_PASSWORD)
    
    # Cria/limpa o arquivo CSV com cabe√ßalho
    with open(CSV_FILENAME, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ID Post", "Autor", "Data", "Texto", "Curtidas"])
        writer.writeheader()

    try:
        scraper.login()
        
        # Mapeamento e Extra√ß√£o
        links = scraper.get_all_post_links(TARGET_PROFILE, target_count=300)
        
        for i, link in enumerate(links):
            print(f"--- Processando {i+1}/{len(links)} ---")
            new_data = scraper.extract_comments_from_post(link)
            
            # Salva os novos dados imediatamente no CSV (modo 'a' - append)
            if new_data:
                with open(CSV_FILENAME, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=["ID Post", "Autor", "Data", "Texto", "Curtidas"])
                    writer.writerows(new_data)
                print(f"üíæ +{len(new_data)} linhas salvas no CSV.")
            
    finally:
        scraper.close()
        print(f"\nüèÅ Processo de Web Scraping finalizado! Arquivo: {CSV_FILENAME}")

if __name__ == "__main__":
    main()

```
----------


## ü•à Camada Silver: Tratamento e Unifica√ß√£o

A camada **Silver** √© o cora√ß√£o da qualidade de dados deste projeto. Seu objetivo √© transformar os dados brutos e fragmentados da camada Bronze em um dataset unificado, limpo e confi√°vel, pronto para modelagem dimensional.

### üõ†Ô∏è O que o script faz (`etl_bronze_to_silver.py`)

O script de processamento executa uma s√©rie de transforma√ß√µes cr√≠ticas para garantir a integridade dos dados:

1.  **Ingest√£o e Leitura H√≠brida (Smart Encoding):**
    
    -   Resolve o problema de arquivos mistos (alguns em UTF-8, outros em Latin-1/Windows-1252).
        
    -   Utiliza uma l√≥gica `try-except` para detectar automaticamente a codifica√ß√£o correta, eliminando caracteres corrompidos (_mojibake_) como `IPANGUA√É‚Ä°U` -> `IPANGUA√áU`.
        
2.  **Limpeza Rigorosa (Data Quality):**
    
    -   **Remo√ß√£o de Ru√≠do:** Elimina linhas totalmente vazias ou cabe√ßalhos repetidos decorrentes da concatena√ß√£o.
        
    -   **Valida√ß√£o de Completude:** Aplica uma regra de neg√≥cio que descarta registros onde campos chave (`Idade`, `Sexo` ou `Munic√≠pio`) estejam ausentes, garantindo que apenas dados utiliz√°veis avancem.
        
3.  **Tratamento Temporal:**
    
    -   Converte a coluna de texto `data_internacao` para objetos `datetime` reais.
        
    -   Remove datas inv√°lidas (erros de digita√ß√£o ou formata√ß√£o).
        
    -   Ordena todo o dataset cronologicamente (do registro mais antigo ao mais recente).
        
4.  **Padroniza√ß√£o de Sa√≠da:**
    
    -   Unifica todos os arquivos trimestrais em um √∫nico arquivo: `internacoes_unificadas.csv`.
        
    -   Salva no S3 com encoding **UTF-8-SIG** e separador **ponto e v√≠rgula (;)**, facilitando a auditoria visual tanto em ferramentas de c√≥digo (VS Code) quanto em planilhas (Excel/Power BI) sem erros de acentua√ß√£o.

O script de processamento executa uma s√©rie de transforma√ß√µes cr√≠ticas para garantir a integridade dos dados:

1.  **Ingest√£o e Leitura H√≠brida (Smart Encoding):**
    
    -   Resolve o problema de arquivos mistos (alguns em UTF-8, outros em Latin-1/Windows-1252).
        
    -   Utiliza uma l√≥gica `try-except` para detectar automaticamente a codifica√ß√£o correta, eliminando caracteres corrompidos (_mojibake_) como `IPANGUA√É‚Ä°U` -> `IPANGUA√áU`.
        
2.  **Limpeza Rigorosa (Data Quality):**
    
    -   **Remo√ß√£o de Ru√≠do:** Elimina linhas totalmente vazias ou cabe√ßalhos repetidos decorrentes da concatena√ß√£o.
        
    -   **Valida√ß√£o de Completude:** Aplica uma regra de neg√≥cio que descarta registros onde campos chave (`Idade`, `Sexo` ou `Munic√≠pio`) estejam ausentes, garantindo que apenas dados utiliz√°veis avancem.
        
3.  **Tratamento Temporal:**
    
    -   Converte a coluna de texto `data_internacao` para objetos `datetime` reais.
        
    -   Remove datas inv√°lidas (erros de digita√ß√£o ou formata√ß√£o).
        
    -   Ordena todo o dataset cronologicamente (do registro mais antigo ao mais recente).
        
4.  **Padroniza√ß√£o de Sa√≠da:**
    
    -   Unifica todos os arquivos trimestrais em um √∫nico arquivo: `internacoes_unificadas.csv`.
        
    -   Salva no S3 com encoding **UTF-8-SIG** e separador **ponto e v√≠rgula (;)**, facilitando a auditoria visual tanto em ferramentas de c√≥digo (VS Code) quanto em planilhas (Excel/Power BI) sem erros de acentua√ß√£o.

    ### üêç C√≥digo da Etapa Silver (`etl_bronze_to_silver.py`)

Abaixo est√° o c√≥digo completo utilizado para realizar a leitura, corre√ß√£o de _encoding_, limpeza de dados nulos e unifica√ß√£o dos arquivos CSV.

Python

```
import pandas as pd
import boto3
import os
from io import StringIO
from dotenv import load_dotenv
from pathlib import Path

# --- CARREGAMENTO DO .env ---
BASEDIR = Path(__file__).resolve().parent
dotenv_path = BASEDIR / '.env'
load_dotenv(dotenv_path=dotenv_path) 

# --- CONFIGURA√á√ïES ---
AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY_ID')
AWS_SECRET_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')
BUCKET_NAME = os.getenv('AWS_BUCKET_NAME')

S3_BRONZE_PREFIX = 'bronze/'
S3_SILVER_KEY = 'silver/internacoes_unificadas.csv'

def etl_process_v3():
    print("üöÄ [ETL V3] Iniciando: Leitura H√≠brida + Limpeza + Ordena√ß√£o...")
    
    if not BUCKET_NAME:
        print("‚ùå Erro: BUCKET_NAME n√£o encontrado no .env")
        return

    s3 = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)
    
    # 1. Listar arquivos
    try:
        response = s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix=S3_BRONZE_PREFIX)
        arquivos = [obj['Key'] for obj in response.get('Contents', []) 
                    if obj['Key'].lower().endswith('.csv')
                    and 'comentarios' not in obj['Key'].lower()]
    except Exception as e:
        print(f"‚ùå Erro ao listar bucket: {e}")
        return

    print(f"üìÇ Encontrados {len(arquivos)} arquivos CSV.")
    
    lista_dfs = []

    # 2. Loop de Leitura e Limpeza (ARQUIVO POR ARQUIVO)
    for arquivo in arquivos:
        nome_arquivo = arquivo.split('/')[-1]
        try:
            obj = s3.get_object(Bucket=BUCKET_NAME, Key=arquivo)
            raw_data = obj['Body'].read()
            
            # --- CORRE√á√ÉO DE ENCODING (INTELIGENTE) ---
            # Tenta UTF-8 primeiro (padr√£o web). Se falhar, usa Latin-1 (padr√£o legado).
            try:
                content = raw_data.decode('utf-8')
            except UnicodeDecodeError:
                content = raw_data.decode('latin1')
            
            # Tenta ler CSV
            try:
                df_temp = pd.read_csv(StringIO(content), sep=';')
                if len(df_temp.columns) < 2:
                    df_temp = pd.read_csv(StringIO(content), sep=',')
            except:
                 df_temp = pd.read_csv(StringIO(content), sep=',')

            # --- LIMPEZA DE DADOS (DENTRO DO LOOP) ---
            qtd_antes = len(df_temp)

            # 1. Remove linhas totalmente vazias
            df_temp.dropna(how='all', inplace=True)
            
            # 2. Limpeza Cr√≠tica: Se faltar Idade, Sexo ou Munic√≠pio, remove a linha
            # (Normaliza nomes para garantir que encontra as colunas mesmo se mai√∫scula/min√∫scula)
            col_map = {c: c.lower() for c in df_temp.columns}
            cols_criticas = []
            for original, lower in col_map.items():
                if 'idade' in lower or 'sexo' in lower or 'munic' in lower:
                    cols_criticas.append(original)
            
            if cols_criticas:
                df_temp.dropna(subset=cols_criticas, how='any', inplace=True)
            
            # 3. Filtra datas inv√°lidas
            if 'data_internacao' in df_temp.columns:
                 df_temp = df_temp[df_temp['data_internacao'].notna()]

            qtd_depois = len(df_temp)
            removidas = qtd_antes - qtd_depois

            if qtd_depois > 0:
                print(f"   -> Lendo: {nome_arquivo}")
                if removidas > 0:
                    print(f"      üßπ Limpeza: {removidas} linhas incompletas removidas.")
                lista_dfs.append(df_temp)
            else:
                print(f"   ‚ö†Ô∏è ALERTA: Arquivo {nome_arquivo} ficou vazio ap√≥s limpeza.")

        except Exception as e:
            print(f"   ‚ùå Erro em {nome_arquivo}: {e}")

    # 3. Consolida√ß√£o Final
    if lista_dfs:
        df_final = pd.concat(lista_dfs, ignore_index=True)
        print(f"\nüìä Total Bruto: {len(df_final)} linhas.")

        # --- TRATAMENTO DE DATA E ORDENA√á√ÉO ---
        print("‚è≥ Convertendo e ordenando datas...")
        
        # Converte para datetime e remove erros
        df_final['data_internacao'] = pd.to_datetime(df_final['data_internacao'], dayfirst=True, errors='coerce')
        df_final = df_final.dropna(subset=['data_internacao'])
        
        # Ordena cronologicamente
        df_final.sort_values(by='data_internacao', ascending=True, inplace=True)
        
        print(f"‚úÖ Ordena√ß√£o conclu√≠da. Per√≠odo: de {df_final['data_internacao'].min()} at√© {df_final['data_internacao'].max()}")

        # Salva no S3 (Silver)
        print("üíæ Salvando na Silver...")
        csv_buffer = StringIO()
        # Salva como UTF-8-SIG (Universal para Excel) e separado por ponto e v√≠rgula
        df_final.to_csv(csv_buffer, index=False, sep=';', encoding='utf-8-sig', date_format='%d/%m/%Y %H:%M')
        
        s3.put_object(Bucket=BUCKET_NAME, Key=S3_SILVER_KEY, Body=csv_buffer.getvalue())
        print(f"üèÅ SUCESSO! Arquivo final salvo em: {S3_SILVER_KEY}")
    else:
        print("‚ùå Nenhum dado processado.")

if __name__ == "__main__":
    etl_process_v3()
```


## üí¨ Camada Silver: Tratamento de Dados N√£o Estruturados (Coment√°rios)

Nesta etapa, focamos na **Sanitiza√ß√£o** e **Preserva√ß√£o de Contexto** dos coment√°rios extra√≠dos do Instagram. O objetivo √© entregar um texto limpo, mas semanticamente rico, para a Camada Gold.

### üõ†Ô∏è O que foi feito (`etl_comentarios_silver.py`)

Diferente de abordagens tradicionais que removem caracteres especiais, nossa estrat√©gia de engenharia priorizou a qualidade para modelos de IA:

1.  **Preserva√ß√£o de Emojis Nativos (UTF-8):**
    
    -   **Decis√£o:** Optamos por **n√£o converter** os emojis em texto (`:cry:`) nem remov√™-los. Mantivemos os caracteres visuais originais (ex: üò¢, üëè).
        
    -   **Motivo T√©cnico:** Algoritmos modernos de an√°lise de sentimento (como o **pysentimiento**, que usaremos na Gold) possuem pesos espec√≠ficos para os s√≠mbolos gr√°ficos. Manter o emoji original maximiza a precis√£o da detec√ß√£o de emo√ß√µes intensas.
        
2.  **Filtro "Cir√∫rgico" de Scraping (Gatekeeper):**
    
    -   Implementamos regras de **Regex (Express√µes Regulares)** para identificar e descartar artefatos de coleta que sujavam os dados:
        
        -   **Datas Soltas:** Remove linhas que cont√™m apenas metadados de tempo (ex: _"27 de novembro de 2024"_), diferenciando-as de coment√°rios v√°lidos que citam datas.
            
        -   **Interface:** Remove bot√µes capturados como texto ("Seguir", "Responder", "Ver tradu√ß√£o").
            
        -   **Lixo Num√©rico:** Elimina linhas compostas apenas por n√∫meros soltos.
            
3.  **Controle de Qualidade:**
    
    -   **Integridade Temporal:** Registros com data de publica√ß√£o nula ou inv√°lida s√£o descartados imediatamente (`dropna`).
        
    -   **Normaliza√ß√£o:** Convers√£o de "29 curtidas" para inteiros (`29`) e padroniza√ß√£o de datas para `YYYY-MM-DD`.
        

### üêç C√≥digo da Etapa Silver - Coment√°rios (V5)

Python

```
import pandas as pd
import boto3
import os
import re
from io import StringIO
from dotenv import load_dotenv
from pathlib import Path

# --- CONFIGURA√á√ÉO ---
BASEDIR = Path(__file__).resolve().parent
load_dotenv(dotenv_path=BASEDIR / '.env')

AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY_ID')
AWS_SECRET_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')
BUCKET_NAME = os.getenv('AWS_BUCKET_NAME')

S3_INPUT_KEY = 'bronze/comentarios_huol_completo_223posts.csv'
S3_OUTPUT_KEY = 'silver/comentarios_tratados.csv'

# --- FUN√á√ïES DE LIMPEZA ---

def limpar_texto_manter_emoji(texto):
    """
    Limpa formata√ß√£o e links, mas MANT√âM os emojis originais (üò¢) para o VADER.
    """
    if not isinstance(texto, str):
        return ""
    
    # Remove URLs
    texto_sem_link = re.sub(r'http\S+', '', texto)
    
    # Remove quebras de linha e espa√ßos duplos
    return re.sub(r'\s+', ' ', texto_sem_link).strip()

def extrair_numero_curtidas(valor):
    """Extrai apenas o n√∫mero inteiro."""
    numeros = re.findall(r'\d+', str(valor))
    return int(numeros[0]) if numeros else 0

def filtrar_lixo_scraping(df):
    """
    Remove artefatos de coleta (bot√µes, contadores e datas soltas no texto).
    """
    qtd_inicial = len(df)
    
    # 1. Blacklist de Interface
    lixo_interface = [
        'Seguir', 'Responder', 'Ver tradu√ß√£o', 
        'Ocultar', 'Ver todas as respostas', 'Ver respostas'
    ]
    df = df[~df['texto_limpo'].isin(lixo_interface)]
    df = df[~df['texto_limpo'].str.contains('Ver todas as', case=False)]
    
    # 2. Lixo Num√©rico
    df = df[~df['texto_limpo'].str.match(r'^\d+$')]
    
    # 3. Lixo de Datas soltas
    # Regex rigoroso para pegar linhas que s√£o APENAS datas
    padrao_data = r'^[\d\s]*\d{1,2}\s+de\s+[a-zA-Z√ß]+(\s+de\s+\d{4})?.*$'
    
    mask_eh_data = df['texto_limpo'].str.match(padrao_data, case=False)
    mask_eh_curto = df['texto_limpo'].str.len() < 40 # S√≥ deleta se for curto (sem contexto)
    
    df = df[~(mask_eh_data & mask_eh_curto)]
    
    removidos = qtd_inicial - len(df)
    if removidos > 0:
        print(f"      üßπ Faxina de Scraping: {removidos} linhas removidas.")
    
    return df

def etl_comentarios():
    print("üöÄ [ETL Coment√°rios V5] Iniciando (Emojis Nativos + Filtro Data)...")
    
    if not BUCKET_NAME:
        print("‚ùå Erro: BUCKET_NAME n√£o encontrado.")
        return

    s3 = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)
    
    try:
        print(f"üì• Baixando Bronze: {S3_INPUT_KEY}")
        obj = s3.get_object(Bucket=BUCKET_NAME, Key=S3_INPUT_KEY)
        df = pd.read_csv(obj['Body'], encoding='utf-8')
    except Exception as e:
        print(f"‚ùå Erro ao ler arquivo: {e}")
        return

    print("‚öôÔ∏è Aplicando regras de limpeza...")
    
    # A. Limpeza de Texto (Mantendo Emoji)
    df['texto_limpo'] = df['Texto'].apply(limpar_texto_manter_emoji)
    
    # B. Filtro de Lixo de Scraping (Datas, bot√µes)
    df = filtrar_lixo_scraping(df)
    
    # C. Dedup
    df.drop_duplicates(subset=['texto_limpo', 'Autor'], inplace=True)
    
    # D. Tratamento de Data (Remove Vazios)
    df['data_publicacao'] = pd.to_datetime(df['Data'], errors='coerce').dt.date
    df.dropna(subset=['data_publicacao'], inplace=True)
    
    # E. M√©tricas
    df['qtd_curtidas'] = df['Curtidas'].apply(extrair_numero_curtidas)
    
    # F. Sele√ß√£o Final
    df_final = df[['data_publicacao', 'texto_limpo', 'qtd_curtidas', 'ID Post']]
    df_final = df_final[df_final['texto_limpo'] != '']
    
    print(f"üìä Total Final Limpo: {len(df_final)} linhas.")
    
    print(f"\nüíæ Salvando Silver: {S3_OUTPUT_KEY}")
    csv_buffer = StringIO()
    df_final.to_csv(csv_buffer, index=False, sep=';', encoding='utf-8-sig')
    
    s3.put_object(Bucket=BUCKET_NAME, Key=S3_OUTPUT_KEY, Body=csv_buffer.getvalue())
    print("üèÅ Sucesso!")

if __name__ == "__main__":
    etl_comentarios()

```
saida:

```
 Total Final Limpo: 567 linhas.

--- Amostra (Verifique os Emojis) ---
                                         texto_limpo
2  O HUOL √© uma Institui√ß√£o de refer√™ncia, de d√©c...
3  üëèüèºüëèüèº Huol √© uma institui√ß√£o de excel√™ncia. Ass...
4  Muita gratid√£o e respeito por essa institui√ß√£o...
5    Muito orgulho de ser parte desta institui√ß√£o! üôå
6  Gostaria de saber porque que as varizes cirurg...

üíæ Salvando Silver: silver/comentarios_tratados.csv
üèÅ Sucesso!
```


## ü•á Camada Gold: Modelagem e Intelig√™ncia Artificial

A camada **Gold** √© o ponto culminante do Data Lake, onde os dados tratados s√£o refinados para responder √†s perguntas de neg√≥cio. Nesta etapa, implementamos duas frentes de engenharia distintas: **Modelagem Dimensional** (para dados estruturados) e **Deep Learning** (para dados n√£o estruturados).

### 1. Modelagem Dimensional (Star Schema)

Para viabilizar a an√°lise perform√°tica no Power BI, transformamos o dataset plano de interna√ß√µes em um modelo relacional otimizado (**Star Schema**) utilizando o motor anal√≠tico **DuckDB**.

-   **Arquitetura:** Separa√ß√£o entre Fato (m√©tricas) e Dimens√µes (contexto).
    
-   **Tabelas Geradas:**
    
    -   `dim_calendario`: Suporta an√°lise temporal (Ano, M√™s, Semestre).
        
    -   `dim_municipio`: Cat√°logo √∫nico de locais.
        
    -   `dim_especialidade`: Cat√°logo de especialidades m√©dicas.
        
    -   `fato_internacoes`: Tabela central contendo as chaves estrangeiras (FKs) e m√©tricas.
        
-   **Tecnologia:** DuckDB executando SQL em mem√≥ria e exportando para formato **Parquet** (colunar), garantindo alta compress√£o e velocidade.

![Modelagem Dimensional (Star Schema](evidencias/005.jpg)
    

### 2. An√°lise de Sentimento (State-of-the-Art NLP)

Para classificar a percep√ß√£o p√∫blica nos coment√°rios do Instagram, abandonamos abordagens baseadas em l√©xicos simples (como VADER) e implementamos um modelo de **Deep Learning** baseado em Transformers (BERT).

-   **Modelo Utilizado:** `bertweet-pt-sentiment` (via biblioteca `pysentimiento`).
    
-   **Por que essa escolha?**
    
    -   **Nativo em Portugu√™s:** Treinado em milh√µes de tweets brasileiros, entende g√≠rias, ironia e erros gramaticais comuns.
        
    -   **Contexto Real:** Diferencia frases complexas (ex: _"Gostaria de saber por que as varizes est√£o suspensas"_ foi corretamente classificado como **Negativo** com 97% de confian√ßa).
        
    -   **Suporte a Emojis:** Interpreta nativamente s√≠mbolos como üëèüëèüëè (Positivo) e üñ§ (Luto/Negativo) sem necessidade de tradu√ß√£o ou convers√£o.
        

----------

### üêç C√≥digo: Modelagem Star Schema (`gold_star_schema.py`)

Python

```
import duckdb
import boto3
import os
import pandas as pd
from io import BytesIO
from dotenv import load_dotenv
from pathlib import Path

# --- CONFIGURA√á√ÉO ---
BASEDIR = Path(__file__).resolve().parent
load_dotenv(dotenv_path=BASEDIR / '.env')

AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY_ID')
AWS_SECRET_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')
BUCKET_NAME = os.getenv('AWS_BUCKET_NAME')

S3_SILVER_KEY = 'silver/internacoes_unificadas.csv'
S3_GOLD_PREFIX = 'gold/'

def processar_gold_star_schema():
    print("üöÄ [GOLD] Iniciando modelagem Dimensional (Star Schema)...")
    
    if not BUCKET_NAME:
        print("‚ùå Erro: BUCKET_NAME n√£o encontrado.")
        return
    
    # 1. Baixar o CSV da Silver
    s3 = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)
    
    try:
        print("üì• Baixando dados da Silver (Tabel√£o)...")
        obj = s3.get_object(Bucket=BUCKET_NAME, Key=S3_SILVER_KEY)
        # L√™ com Pandas
        df_silver = pd.read_csv(obj['Body'], sep=';', encoding='utf-8')
        
        # --- CORRE√á√ÉO DE CABE√áALHO ---
        # Renomeia 'munic√≠pio' para 'municipio' para facilitar o SQL
        df_silver.rename(columns={'munic√≠pio': 'municipio'}, inplace=True)
        
        # Garante que a data seja data mesmo
        df_silver['data_internacao'] = pd.to_datetime(df_silver['data_internacao'], dayfirst=True)
        
    except Exception as e:
        print(f"‚ùå Erro ao ler a Silver: {e}")
        return
    
    # 2. Iniciar DuckDB
    print("ü¶Ü Iniciando motor SQL (DuckDB)...")
    con = duckdb.connect(database=':memory:')
    con.register('tb_silver', df_silver)
    
    # ---------------------------------------------------------
    # 3. CRIAR DIMENS√ïES (SQL)
    # ---------------------------------------------------------
    
    print("üî® [1/4] Criando Dimens√£o: Especialidade...")
    con.execute("""
        CREATE TABLE dim_especialidade AS
        SELECT 
            row_number() OVER (ORDER BY especialidade) AS id_especialidade,
            especialidade AS nome_especialidade
        FROM (SELECT DISTINCT especialidade FROM tb_silver WHERE especialidade IS NOT NULL)
    """)
    
    print("üî® [2/4] Criando Dimens√£o: Munic√≠pio...")
    # Agora usamos 'municipio' sem acento
    con.execute("""
        CREATE TABLE dim_municipio AS
        SELECT 
            row_number() OVER (ORDER BY municipio) AS id_municipio,
            municipio AS nome_municipio
        FROM (SELECT DISTINCT municipio FROM tb_silver WHERE municipio IS NOT NULL)
    """)
    
    print("üî® [3/4] Criando Dimens√£o: Calend√°rio...")
    con.execute("""
        CREATE TABLE dim_calendario AS
        SELECT DISTINCT
            data_internacao AS id_calendario,
            YEAR(data_internacao) AS ano,
            MONTH(data_internacao) AS mes,
            DAY(data_internacao) AS dia,
            CASE 
                WHEN MONTH(data_internacao) <= 6 THEN 1 
                ELSE 2 
            END AS semestre
        FROM tb_silver
        WHERE data_internacao IS NOT NULL
        ORDER BY 1
    """)

    # ---------------------------------------------------------
    # 4. CRIAR FATO
    # ---------------------------------------------------------
    print("üî® [4/4] Criando Tabela Fato: Interna√ß√µes...")
    con.execute("""
        CREATE TABLE fato_internacoes AS
        SELECT 
            s.data_internacao AS id_calendario,
            m.id_municipio,
            e.id_especialidade,
            s.idade,
            s.sexo,
            1 AS qtd_internacao
        FROM tb_silver s
        LEFT JOIN dim_municipio m ON s.municipio = m.nome_municipio
        LEFT JOIN dim_especialidade e ON s.especialidade = e.nome_especialidade
    """)
    
    # ---------------------------------------------------------
    # 5. SALVAR NO S3
    # ---------------------------------------------------------
    print("üíæ Salvando tabelas na camada Gold (Parquet)...")
    
    tabelas = ['dim_especialidade', 'dim_municipio', 'dim_calendario', 'fato_internacoes']
    
    for tabela in tabelas:
        df_export = con.table(tabela).df()
        
        parquet_buffer = BytesIO()
        df_export.to_parquet(parquet_buffer, index=False)
        
        key = f"{S3_GOLD_PREFIX}{tabela}.parquet"
        s3.put_object(Bucket=BUCKET_NAME, Key=key, Body=parquet_buffer.getvalue())
        print(f"   ‚úÖ {tabela} salva em: {key}")

    print("üèÅ Sucesso! Data Warehouse criado.")

if __name__ == "__main__":
    processar_gold_star_schema()

```

### üêç C√≥digo: An√°lise de Sentimento com BERT (`analise_sentimento_gold.py`)

Python

```
import pandas as pd
import boto3
import os
from io import StringIO
from dotenv import load_dotenv
from pathlib import Path
from pysentimiento import create_analyzer

# --- CONFIGURA√á√ÉO ---
BASEDIR = Path(__file__).resolve().parent
load_dotenv(dotenv_path=BASEDIR / '.env')

AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY_ID')
AWS_SECRET_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')
BUCKET_NAME = os.getenv('AWS_BUCKET_NAME')

S3_INPUT_KEY = 'silver/comentarios_tratados.csv'
S3_OUTPUT_KEY = 'gold/comentarios_sentimento_bert.csv'

def processar_com_bert():
    print("üöÄ [GOLD] Iniciando An√°lise com BERT (Pysentimiento)...")
    
    # 1. Carrega o Modelo (BERTweet-PT)
    print("üß† Carregando modelo 'bertweet-pt-sentiment'...")
    analyzer = create_analyzer(task="sentiment", lang="pt")

    # 2. Conecta no S3
    s3 = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)
    print("üì• Baixando dados da Silver...")
    obj = s3.get_object(Bucket=BUCKET_NAME, Key=S3_INPUT_KEY)
    df = pd.read_csv(obj['Body'], sep=';', encoding='utf-8')

    # 3. Processamento
    print("‚öôÔ∏è Analisando sentimentos (Deep Learning)...")
    
    sentimentos = []
    scores = []
    
    total = len(df)
    for i, row in df.iterrows():
        texto = row['texto_limpo']
        
        # O modelo processa direto o texto em PT com emojis
        resultado = analyzer.predict(texto)
        
        # O resultado vem como "POS", "NEG", "NEU" -> Mapeamos para PT
        mapa = {'POS': 'Positivo', 'NEG': 'Negativo', 'NEU': 'Neutro'}
        sentimentos.append(mapa[resultado.output])
        
        # Probabilidade da classe escolhida
        probabilidade = resultado.probas[resultado.output]
        
        # Ajuste de sinal para visualiza√ß√£o (-1 a 1)
        if resultado.output == 'NEG':
            scores.append(probabilidade * -1)
        elif resultado.output == 'NEU':
            scores.append(0.0)
        else:
            scores.append(probabilidade)

        if i % 50 == 0:
            print(f"   ... Processado {i}/{total}")

    df['sentimento'] = sentimentos
    df['score_sentimento'] = scores
    
    # 4. Salvar
    print(f"\nüíæ Salvando em: {S3_OUTPUT_KEY}")
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False, sep=';', encoding='utf-8-sig')
    s3.put_object(Bucket=BUCKET_NAME, Key=S3_OUTPUT_KEY, Body=csv_buffer.getvalue())
    print("üèÅ Sucesso!")

if __name__ == "__main__":
    processar_com_bert()
```

Saida
```
--- Amostra Final (BERT) ---
                                         texto_limpo sentimento  score_sentimento
0  O HUOL √© uma Institui√ß√£o de refer√™ncia, de d√©c...     Neutro          0.000000
1  üëèüèºüëèüèº Huol √© uma institui√ß√£o de excel√™ncia. Ass...   Positivo          0.830135
2  Muita gratid√£o e respeito por essa institui√ß√£o...   Positivo          0.990246
3    Muito orgulho de ser parte desta institui√ß√£o! üôå   Positivo          0.988792
4  Gostaria de saber porque que as varizes cirurg...   Negativo         -0.972792
5  Parab√©ns a todos que fazem do HUOL, uma instit...   Positivo          0.976684
6  O HUOL √© uma institui√ß√£o centen√°ria e de refer...     Neutro          0.000000
7                                                üëèüëèüëè   Positivo          0.760595

üìä Resumo:
sentimento
Positivo    337
Neutro      127
Negativo    103
Name: count, dtype: int64

üíæ Salvando em: gold/comentarios_sentimento_bert.csv
üèÅ Sucesso!
```


## ‚òÅÔ∏è Configura√ß√£o do Data Warehouse Serverless (AWS)

Para profissionalizar o acesso aos dados e permitir consultas SQL diretas sobre o Data Lake, implementamos uma arquitetura _serverless_ utilizando **AWS Glue** e **AWS Athena**. Isso elimina a necessidade de carregar arquivos manualmente e cria uma camada de abstra√ß√£o robusta para o Power BI.

### 1. Cataloga√ß√£o de Dados (AWS Glue)

O **AWS Glue Crawler** foi utilizado para escanear automaticamente a camada Gold no S3, inferir o esquema dos arquivos (Parquet e CSV) e criar as tabelas no Data Catalog.

![AWS Glue Crawler](evidencias/006.jpg)

-   **Crawler Name:** `crawler_hospital_gold`
    
-   **Data Source:** `s3://datalake-sad-huol-weillon/gold/`
    
-   **Output Database:** `db_hospital_gold`
    
-   **IAM Role:** `AWSGlueServiceRole-hospital-acesso` (Configurada com permiss√£o de leitura/escrita no bucket espec√≠fico).
    
-   **Schedule:** _On Demand_ (Execu√ß√£o manual para otimiza√ß√£o de custos).
    

> **Estrat√©gia de Organiza√ß√£o:** Para garantir a correta infer√™ncia dos formatos (Parquet vs CSV), os arquivos no S3 foram organizados em subpastas dedicadas (`gold/fato_internacoes/`, `gold/comentarios/`, etc.), respeitando a regra de "uma tabela por pasta" do Glue.

### 2. Motor de Consulta (AWS Athena)

O **AWS Athena** atua como a interface SQL para os dados armazenados no S3, permitindo queries interativas e servindo como backend para ferramentas de BI.

![AWS Athena](evidencias/007.jpg)

-   **Configura√ß√£o Obrigat√≥ria:** Defini√ß√£o do _Query Result Location_ no S3 (`s3://.../athena-results/`) para armazenar os metadados das consultas.
    
-   **Valida√ß√£o:** Testes de consist√™ncia realizados via SQL (`SELECT * FROM ...`) para garantir que os dados de sentimento e m√©tricas hospitalares estavam acess√≠veis e tipados corretamente.
    
    

### 3. Conectividade (ODBC & Power BI)

A integra√ß√£o final com o dashboard foi realizada atrav√©s do **Driver ODBC Amazon Athena**, proporcionando uma conex√£o segura e perform√°tica.

-   **Driver:** Simba Athena ODBC Driver (64-bit).
    
-   **Autentica√ß√£o:** IAM Credentials (Access Key / Secret Key).
    
-   **DSN (Data Source Name):** `AthenaHospital`.
    
-   **Vantagem:** Permite que o Power BI utilize o modo **Import** ou **DirectQuery**, delegando o processamento pesado para a nuvem AWS em vez da m√°quina local.


## ‚öôÔ∏è Integra√ß√£o Final: Cloud Data Warehouse (AWS Glue & Athena)

A etapa final do projeto garante que o Power BI possa consumir os dados modelados (Camada Gold) via SQL de forma perform√°tica e segura, sem depender de downloads locais.

### 1. Cataloga√ß√£o Serverless (AWS Glue)

O cora√ß√£o da integra√ß√£o √© o **AWS Glue Data Catalog**. O Crawler (`crawler_hospital_gold`) foi configurado para escanear a estrutura organizada em S3 (pastas `gold/...`) e inferir automaticamente o esquema das tabelas (identificando campos como `int`, `string` e `timestamp` nos arquivos Parquet e CSV).

-   **Resultado:** Cria√ß√£o do banco de dados `db_hospital_gold` e das 5 tabelas (Dimens√µes, Fato e Sentimento), tornando o S3 consult√°vel via SQL.
    

### 2. Motor de Consulta e Conex√£o (Athena & ODBC)

O AWS Athena foi configurado como o motor de consulta _serverless_.

![ODBC](evidencias/008.jpg)


-   **Query Engine:** Athena executa consultas SQL diretamente sobre os arquivos Parquet/CSV no S3, eliminando a necessidade de um servidor de banco de dados tradicional.
    
-   **Conectividade:** O Power BI Desktop foi conectado ao Athena utilizando o **Driver ODBC (Simba Athena)**, autenticado com as credenciais **IAM** do usu√°rio.
    
-   **Resultado Final:** O dashboard no Power BI Desktop consome os dados em _streaming_ direto da nuvem, validando a arquitetura ponta a ponta.



## üìä Visualiza√ß√£o e Estrat√©gia de Neg√≥cio (Dashboard)

Com a infraestrutura de dados validada e conectada, a etapa final consistiu na constru√ß√£o do painel de Business Intelligence. O objetivo central n√£o foi apenas apresentar gr√°ficos, mas transformar os dados processados em uma ferramenta de **Apoio √† Decis√£o** (Decision Support System) para a diretoria do hospital.

### üéØ Motiva√ß√£o e Foco Estrat√©gico

A constru√ß√£o do dashboard foi estritamente guiada pelas perguntas de neg√≥cio estabelecidas no planejamento. Essa abordagem evita o desenvolvimento de m√©tricas de vaidade e garante que cada visualiza√ß√£o tenha um prop√≥sito claro: **subsidiar a viabilidade do credenciamento ao SUS**.

O painel foi estruturado para responder √†s seguintes **6 Perguntas Chave**:

1.  **Quais especialidades hospitalares concentram o maior n√∫mero de interna√ß√µes?**
    
    -   _Impacto:_ Define o foco operacional e aloca√ß√£o de recursos.
        
2.  **Qual o perfil et√°rio e de g√™nero dos pacientes internados?**
    
    -   _Impacto:_ Planejamento de leitos e especialidades (ex: Pediatria vs Geriatria).
        
3.  **Existem padr√µes por munic√≠pio (ex.: mais interna√ß√µes de residentes de Natal ou do interior)?**
    
    -   _Impacto:_ Entendimento da abrang√™ncia regional e log√≠stica.
        
4.  **H√° sazonalidade nas interna√ß√µes (varia√ß√£o entre os meses de 2024 e 2025)?**
    
    -   _Impacto:_ Previs√£o de demanda e gest√£o de escalas de plant√£o.
        
5.  **O que as pessoas est√£o dizendo sobre o hospital?**
    
    -   _Impacto:_ An√°lise qualitativa da imagem institucional.
        
6.  **Quais percep√ß√µes ou sentimentos predominam nos coment√°rios do Instagram (positivos, negativos, neutros)?**
    
    -   _Impacto:_ KPI de reputa√ß√£o baseado em Intelig√™ncia Artificial.
        

### üìà Implementa√ß√£o Visual (Solu√ß√£o)

Para responder a essas quest√µes, o dashboard no Power BI foi dividido em tr√™s √°reas estrat√©gicas, utilizando os dados modelados no **Star Schema** e enriquecidos com **NLP**:

-   **Demanda e Geografia (Q1 e Q3):** Utiliza√ß√£o de **Gr√°ficos de Barras (Top N)** e **Treemaps** para evidenciar a alta concentra√ß√£o de atendimentos na Regi√£o Metropolitana e o ranking das especialidades mais buscadas.
    
-   **Perfil e Tend√™ncia (Q2 e Q4):** Implementa√ß√£o de uma **Pir√¢mide Et√°ria** din√¢mica e **Gr√°ficos de Linha** temporais, permitindo a compara√ß√£o de sazonalidade entre os anos fiscais.
![vq](evidencias/009.jpg)
    
-   **Intelig√™ncia Artificial (Q5 e Q6):** Aplica√ß√£o dos resultados do modelo **BERT**, visualizados atrav√©s de gr√°ficos de **Rosca (Sentimento Predominante)** e tabelas detalhadas que exp√µem as principais cr√≠ticas e elogios extra√≠dos das redes sociais.

![v2](evidencias/010.jpg)

## ‚ú® Resultados e Insights

A arquitetura combina dados estruturados (governamentais) e n√£o estruturados (redes sociais) em um **Data Lakehouse Serverless na AWS**, utilizando Intelig√™ncia Artificial para an√°lise de reputa√ß√£o. 


### üìä Vis√£o Geral e Demanda

-   **Top 10 Especialidades:** Identificamos as √°reas de maior press√£o de demanda no SUS.
    
-   **Geografia:** O mapa de calor (Treemap) revelou que a demanda √© altamente concentrada na Regi√£o Metropolitana (Natal e Parnamirim).
    

### üë• Perfil do Paciente

-   **Pir√¢mide Et√°ria:** An√°lise detalhada por g√™nero e faixas et√°rias de 10 anos.
    
-   **Sazonalidade:** Monitoramento mensal comparativo (2024 vs 2025) para prever picos de ocupa√ß√£o.
    

### üß† A Voz das Redes

-   **Sentimento:** O modelo BERT identificou uma predomin√¢ncia de **59% de Elogios**, validando a boa reputa√ß√£o do hospital.
    
-   **Qualitativo:** Tabela detalhada filtrando as "Top Cr√≠ticas" para a√ß√£o imediata da gest√£o.

Explore os resultados da an√°lise no dashboard interativo a seguir.

üëâ **[Acesse o Dashboard Interativo ](https://app.powerbi.com/view?r=eyJrIjoiNTVmYzIxNWQtYWNkYy00M2FmLWE1OTYtZDVhMTNiMzkxYmZjIiwidCI6IjE5OTA0MTBmLTJlYzctNDIyZi1iNmY3LTMzNDVkMGJjNTMzMyJ9)**

---
üé• V√≠deo Explicativo
Assista a uma apresenta√ß√£o completa do projeto, desde o ETL, implementa√ß√£o na nuvem, at√© o dashboard final:

üëâ [Apresenta√ß√£o do Projeto no YouTube](xxxxxxxxxxxxxxxxxxx)

---
## üöÄ Tecnologias Utilizadas

-   **Cloud:** AWS S3, AWS Glue, AWS Athena.
    
-   **Linguagem:** Python 3.10+.
    
-   **Bibliotecas:** Pandas, Boto3, DuckDB, Pysentimiento (Transformers/BERT).
    
-   **Visualiza√ß√£o:** Microsoft Power BI (Conector ODBC).
    

----------

## üë®‚Äçüíª Autor

**Weillon Mota**

-   [LinkedIn](https://www.linkedin.com/in/weillonmota/)
    
-   [GitHub](https://github.com/weillonmota/projetos)
