{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b2765d",
   "metadata": {},
   "source": [
    "# ü§ñ Prevendo Notas do ENEM com Machine Learning\n",
    "\n",
    "Bem-vindo! Este projeto √© um guia pr√°tico que demonstra como usar dados para treinar um modelo de Machine Learning capaz de prever as notas de um estudante no ENEM. Utilizamos uma t√©cnica chamada **k-Nearest Neighbors (k-NN)**, ou \"k-Vizinhos Mais Pr√≥ximos\", para realizar essa tarefa.\n",
    "\n",
    "O objetivo √© criar um material did√°tico, explicando cada passo do processo de forma clara e simples, desde a coleta dos dados brutos at√© o salvamento de um modelo pronto para uso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2818ea",
   "metadata": {},
   "source": [
    "## üìÇ Estrutura do Projeto\n",
    "\n",
    "O c√≥digo est√° organizado em um notebook Jupyter (`etl.ipynb`) e dividido em quatro etapas principais:\n",
    "\n",
    "1.  **Configura√ß√£o**: Onde preparamos nosso ambiente e definimos as regras do jogo.\n",
    "2.  **ETL (Extra√ß√£o, Transforma√ß√£o e Carga)**: A fase de \"faxina\", onde limpamos e organizamos os dados.\n",
    "3.  **Otimiza√ß√£o do k-NN**: O cora√ß√£o do projeto, onde ensinamos o modelo a aprender com os dados.\n",
    "4.  **Treinamento Final**: Onde consolidamos o aprendizado e salvamos nosso modelo inteligente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca3dea",
   "metadata": {},
   "source": [
    "## ü§î Como o Programa Funciona? Um Guia Passo a Passo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4a679",
   "metadata": {},
   "source": [
    "### Etapa 1: Configura√ß√£o e Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c907999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ M√≥dulo de Configura√ß√£o carregado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Importa√ß√£o de bibliotecas essenciais\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# --- ARQUIVOS E CAMINHOS ---\n",
    "ARQUIVOS_ENEM = ['MICRODADOS_ENEM_2021.csv', 'MICRODADOS_ENEM_2022.csv', 'MICRODADOS_ENEM_2023.csv']\n",
    "ARQUIVO_DADOS_CEARA = 'dados_ceara.csv'\n",
    "ARQUIVO_MODELO = 'modelo_knn.joblib'\n",
    "ARQUIVO_COLUNAS = 'colunas_modelo.json'\n",
    "\n",
    "# --- COLUNAS DO DATASET ---\n",
    "# Colunas que ser√£o o alvo da nossa previs√£o (target)\n",
    "COLUNAS_ALVO = ['NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT', 'NU_NOTA_REDACAO']\n",
    "# Colunas que usaremos para fazer a previs√£o (features)\n",
    "COLUNAS_FEATURES = ['Q006', 'Q002', 'TP_ESCOLA', 'TP_COR_RACA', 'SG_UF_ESC']\n",
    "# Lista completa de colunas a serem lidas dos arquivos originais\n",
    "COLUNAS_DESEJADAS = ['NU_ANO'] + COLUNAS_ALVO + COLUNAS_FEATURES\n",
    "\n",
    "# --- PAR√ÇMETROS DO MODELO ---\n",
    "ESTADO_ALEATORIO = 42 # Garante que a divis√£o dos dados seja sempre a mesma\n",
    "TAMANHO_TESTE = 0.2  # Define que 20% dos dados ser√£o usados para teste\n",
    "\n",
    "print(\"‚úÖ M√≥dulo de Configura√ß√£o carregado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8bf0eb",
   "metadata": {},
   "source": [
    "Antes de come√ßar qualquer projeto de programa√ß√£o, √© uma boa pr√°tica organizar as ferramentas. Nesta primeira c√©lula do c√≥digo, n√≥s:\n",
    "- **Importamos as bibliotecas**: Ferramentas como `pandas` (para mexer com tabelas) e `scikit-learn` (para Machine Learning).\n",
    "- **Centralizamos as informa√ß√µes**: Definimos nomes de arquivos e colunas em um √∫nico lugar. Isso √© como ter uma \"lista de compras\" antes de ir ao mercado; se precisarmos mudar algo, sabemos exatamente onde olhar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa88d58c",
   "metadata": {},
   "source": [
    "### Etapa 2: A Faxina dos Dados (ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6ed305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üöÄ Iniciando processo de ETL para o estado: CE ---\n",
      "Processando: MICRODADOS_ENEM_2021.csv...\n",
      "-> 58503 linhas v√°lidas encontradas.\n",
      "Processando: MICRODADOS_ENEM_2022.csv...\n",
      "-> 71340 linhas v√°lidas encontradas.\n",
      "Processando: MICRODADOS_ENEM_2023.csv...\n",
      "-> 74319 linhas v√°lidas encontradas.\n",
      "\n",
      "‚úÖ ETL Conclu√≠do. DataFrame final criado com 204162 linhas.\n",
      "üíæ Dados limpos salvos em 'dados_ceara.csv'.\n"
     ]
    }
   ],
   "source": [
    "def executar_etl(lista_arquivos: list, colunas: list, estado_filtro: str = 'CE') -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    L√™, limpa, filtra e combina m√∫ltiplos arquivos CSV do ENEM.\n",
    "    - Extra√ß√£o: L√™ apenas as colunas desejadas para otimizar o uso de mem√≥ria.\n",
    "    - Transforma√ß√£o: Remove linhas com dados faltantes e filtra pelo estado.\n",
    "    - Carga: Consolida os dados de todos os arquivos em um √∫nico DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- üöÄ Iniciando processo de ETL para o estado: {estado_filtro} ---\")\n",
    "    dataframes = []\n",
    "\n",
    "    for arquivo in lista_arquivos:\n",
    "        if not os.path.exists(arquivo):\n",
    "            print(f\"‚ö†Ô∏è AVISO: Arquivo '{arquivo}' n√£o encontrado. Pulando...\")\n",
    "            continue\n",
    "        try:\n",
    "            print(f\"Processando: {arquivo}...\")\n",
    "            df = pd.read_csv(arquivo, sep=';', encoding='latin-1', usecols=colunas)\n",
    "            df.dropna(inplace=True)\n",
    "            df_estado = df[df['SG_UF_ESC'] == estado_filtro]\n",
    "            if not df_estado.empty:\n",
    "                dataframes.append(df_estado)\n",
    "                print(f\"-> {len(df_estado)} linhas v√°lidas encontradas.\")\n",
    "            else:\n",
    "                print(\"-> Nenhuma linha v√°lida para o Cear√° neste arquivo.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERRO inesperado ao processar '{arquivo}': {e}\")\n",
    "\n",
    "    if not dataframes:\n",
    "        print(\"\\n‚ùå ETL falhou. Nenhum DataFrame foi processado.\")\n",
    "        return None\n",
    "\n",
    "    df_final = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\n‚úÖ ETL Conclu√≠do. DataFrame final criado com {len(df_final)} linhas.\")\n",
    "    df_final.to_csv(ARQUIVO_DADOS_CEARA, index=False)\n",
    "    print(f\"üíæ Dados limpos salvos em '{ARQUIVO_DADOS_CEARA}'.\")\n",
    "    return df_final\n",
    "\n",
    "# Executa a fun√ß√£o\n",
    "dados_limpos_ceara = executar_etl(lista_arquivos=ARQUIVOS_ENEM, colunas=COLUNAS_DESEJADAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8048430b",
   "metadata": {},
   "source": [
    "Os dados brutos do ENEM s√£o gigantescos e um pouco bagun√ßados. Para que nosso modelo consiga aprender algo √∫til, precisamos primeiro organizar a casa. O processo de ETL faz exatamente isso:\n",
    "\n",
    "1.  **Extrair**: Lemos os arquivos CSV do ENEM (`MICRODADOS_ENEM_2021.csv`, etc.). Para n√£o sobrecarregar a mem√≥ria do computador, pegamos apenas as colunas que nos interessam.\n",
    "2.  **Transformar**: Aqui acontece a m√°gica da limpeza. N√≥s:\n",
    "    - Removemos todas as linhas que t√™m alguma informa√ß√£o faltando (como uma nota em branco).\n",
    "    - Filtramos os dados para manter apenas os registros de estudantes do Cear√° (`CE`).\n",
    "3.  **Carregar**: Juntamos todos os dados limpos de diferentes anos em uma √∫nica tabela e a salvamos como `dados_ceara.csv`. Ter esse arquivo limpo economiza muito tempo, pois n√£o precisamos repetir essa faxina toda vez que rodamos o projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3069e",
   "metadata": {},
   "source": [
    "### Etapa 3: Ensinando o Computador a \"Adivinhar\" com k-NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b36d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üß† Buscando o melhor valor de 'k' para o k-NN ---\n",
      "Iniciando os testes (esta etapa pode demorar)...\n",
      "   - Processando k = 3...\n",
      "     -> Teste k=3 conclu√≠do! MAE Geral: 95.0777\n",
      "   - Processando k = 5...\n",
      "     -> Teste k=5 conclu√≠do! MAE Geral: 96.2481\n",
      "   - Processando k = 7...\n",
      "     -> Teste k=7 conclu√≠do! MAE Geral: 92.9195\n",
      "   - Processando k = 9...\n",
      "     -> Teste k=9 conclu√≠do! MAE Geral: 90.2997\n",
      "   - Processando k = 11...\n",
      "     -> Teste k=11 conclu√≠do! MAE Geral: 89.7812\n",
      "   - Processando k = 13...\n",
      "     -> Teste k=13 conclu√≠do! MAE Geral: 88.5116\n",
      "   - Processando k = 15...\n",
      "     -> Teste k=15 conclu√≠do! MAE Geral: 88.0135\n",
      "   - Processando k = 17...\n",
      "     -> Teste k=17 conclu√≠do! MAE Geral: 88.0067\n",
      "   - Processando k = 19...\n",
      "     -> Teste k=19 conclu√≠do! MAE Geral: 87.9317\n",
      "\n",
      "   -> Todos os testes foram conclu√≠dos.\n",
      "\n",
      "‚úÖ Otimiza√ß√£o Finalizada. Melhor valor encontrado: k = 19\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time # Usaremos para dar uma sensa√ß√£o de progresso\n",
    "\n",
    "def encontrar_melhor_k(df: pd.DataFrame, valores_k: list) -> int:\n",
    "    \"\"\"\n",
    "    Testa diferentes valores de 'k' para o k-NN e retorna o que minimiza o erro,\n",
    "    com prints detalhados do progresso.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- üß† Buscando o melhor valor de 'k' para o k-NN ---\")\n",
    "    if df is None:\n",
    "        print(\"‚ùå DataFrame de entrada √© inv√°lido. Abortando a otimiza√ß√£o.\")\n",
    "        return 0\n",
    "\n",
    "    # 1. Preparando a busca pelo melhor valor de 'k'\")\n",
    "    \n",
    "    X = pd.get_dummies(df[COLUNAS_FEATURES], drop_first=True)\n",
    "    y = df[COLUNAS_ALVO]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TAMANHO_TESTE, random_state=ESTADO_ALEATORIO\n",
    "    )\n",
    "    \n",
    "    print(\"Iniciando os testes (esta etapa pode demorar)...\")\n",
    "    resultados_mae = {}\n",
    "    for k in valores_k:\n",
    "        # Print que mostra o in√≠cio do teste para um 'k' espec√≠fico\n",
    "        print(f\"   - Processando k = {k}...\")\n",
    "        \n",
    "        # Estas s√£o as linhas que demoram:\n",
    "        modelo = KNeighborsRegressor(n_neighbors=k, n_jobs=-1)\n",
    "        modelo.fit(X_train, y_train)\n",
    "        y_pred = modelo.predict(X_test)\n",
    "        \n",
    "        # C√°lculo do resultado\n",
    "        mae_geral = mean_absolute_error(y_test, y_pred)\n",
    "        resultados_mae[k] = mae_geral\n",
    "        \n",
    "        # Print que mostra o resultado parcial ao final de cada teste\n",
    "        print(f\"     -> Teste k={k} conclu√≠do! MAE Geral: {mae_geral:.4f}\")\n",
    "\n",
    "    # Prints finais\n",
    "    melhor_k = min(resultados_mae, key=resultados_mae.get)\n",
    "    print(\"\\n   -> Todos os testes foram conclu√≠dos.\")\n",
    "    print(f\"\\n‚úÖ Otimiza√ß√£o Finalizada. Melhor valor encontrado: k = {melhor_k}\")\n",
    "    return melhor_k\n",
    "\n",
    "# --- Execu√ß√£o da Fun√ß√£o ---\n",
    "# Garante que os dados existem antes de rodar\n",
    "if 'dados_limpos_ceara' in locals():\n",
    "    valores_k_para_testar = [3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
    "    melhor_k = encontrar_melhor_k(dados_limpos_ceara, valores_k_para_testar)\n",
    "else:\n",
    "    print(\"‚ùå ERRO: A vari√°vel 'dados_limpos_ceara' n√£o foi encontrada. Execute a c√©lula de ETL primeiro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad32214",
   "metadata": {},
   "source": [
    "Agora chegamos √† parte mais interessante: como o modelo de Machine Learning realmente funciona? Usamos o algoritmo **k-Nearest Neighbors (k-NN)**, que √© surpreendentemente intuitivo.\n",
    "\n",
    "#### Uma Analogia Simples para Entender o k-NN\n",
    "\n",
    "Imagine que voc√™ quer prever o pre√ßo de uma casa, mas n√£o tem ideia de como fazer isso. Uma abordagem simples seria:\n",
    "1.  Encontrar as **3 casas mais parecidas** com a sua na mesma vizinhan√ßa (com tamanho, n√∫mero de quartos e idade similares).\n",
    "2.  Perguntar o pre√ßo de cada uma dessas 3 casas.\n",
    "3.  Calcular a **m√©dia de pre√ßo** dessas 3 casas.\n",
    "4.  Pronto! Essa m√©dia √© a sua \"previs√£o\" para o pre√ßo da sua casa.\n",
    "\n",
    "O k-NN faz exatamente isso, mas para prever as notas do ENEM. Ele n√£o prev√™ o pre√ßo de uma casa, mas sim as notas de um aluno (`NU_NOTA_CN`, `NU_NOTA_MT`, etc.).\n",
    "\n",
    "#### Como o k-NN Foi Implementado Neste Projeto?\n",
    "\n",
    "1.  **Definindo os \"Vizinhos\" (as `COLUNAS_FEATURES`)**: Para o nosso modelo, um \"vizinho\" √© um outro estudante com um perfil socioecon√¥mico parecido. N√≥s dizemos ao algoritmo para comparar os alunos com base em caracter√≠sticas como:\n",
    "    - `Q006`: A renda familiar.\n",
    "    - `Q002`: A escolaridade da m√£e.\n",
    "    - `TP_ESCOLA`: O tipo de escola (p√∫blica ou privada).\n",
    "\n",
    "2.  **O \"k\" da Quest√£o (A Otimiza√ß√£o do Hiperpar√¢metro)**: Na nossa analogia, usamos 3 casas. Mas por que 3? Por que n√£o 5, 10 ou 20? Esse n√∫mero √© o `k`, e escolher o `k` certo √© fundamental.\n",
    "    - Se `k` for **muito pequeno** (ex: `k=1`), o modelo fica \"superespecialista\". Ele baseia a previs√£o em um √∫nico vizinho, o que pode ser muito inst√°vel e levar a erros.\n",
    "    - Se `k` for **muito grande** (ex: `k=100`), o modelo fica \"gen√©rico demais\". Ele considera tantos vizinhos que a previs√£o se torna uma m√©dia muito geral, perdendo as particularidades.\n",
    "\n",
    "    A c√©lula **\"Otimiza√ß√£o do Hiperpar√¢metro 'k'\"** automatiza essa busca. Ela testa v√°rios valores para `k` (3, 5, 7, 9, etc.) e, para cada um, calcula o **Erro M√©dio Absoluto (MAE)**. O MAE nos diz, em m√©dia, qu√£o longe as previs√µes do modelo ficaram das notas reais. O valor de `k` que gerar o menor erro √© o campe√£o!\n",
    "\n",
    "3.  **O Modelo em A√ß√£o (`KNeighborsRegressor`)**: A ferramenta que usamos para isso √© o `KNeighborsRegressor` do `scikit-learn`. O nome \"Regressor\" indica que ele serve para prever n√∫meros cont√≠nuos (como uma nota de 0 a 1000), e n√£o para classificar em categorias (como \"aprovado\" ou \"reprovado\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28190604",
   "metadata": {},
   "source": [
    "### Etapa 4: Treinamento e Salvamento do Modelo Final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c326f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üöÇ Treinando e salvando o modelo final com k = 19 ---\n",
      "‚úÖ Modelo final treinado com sucesso!\n",
      "üíæ Modelo salvo em: 'modelo_knn.joblib'\n",
      "üíæ Colunas salvas em: 'colunas_modelo.json'\n",
      "\n",
      "--- üöÄ PROJETO CONCLU√çDO ---\n"
     ]
    }
   ],
   "source": [
    "def treinar_e_salvar_modelo_final(df: pd.DataFrame, k: int):\n",
    "    \"\"\"\n",
    "    Treina o modelo k-NN com o valor de 'k' otimizado e salva os artefatos.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- üöÇ Treinando e salvando o modelo final com k = {k} ---\")\n",
    "    if df is None or k == 0:\n",
    "        print(\"‚ùå Dados ou valor de 'k' inv√°lidos. Abortando treinamento.\")\n",
    "        return\n",
    "\n",
    "    X = pd.get_dummies(df[COLUNAS_FEATURES], drop_first=True)\n",
    "    y = df[COLUNAS_ALVO]\n",
    "    \n",
    "    # Dica: O modelo final pode ser treinado com todos os dados, mas para manter\n",
    "    # a consist√™ncia do projeto, vamos usar o mesmo split.\n",
    "    X_train, _, y_train, _ = train_test_split(\n",
    "        X, y, test_size=TAMANHO_TESTE, random_state=ESTADO_ALEATORIO\n",
    "    )\n",
    "\n",
    "    modelo_final = KNeighborsRegressor(n_neighbors=k, n_jobs=-1)\n",
    "    modelo_final.fit(X_train, y_train)\n",
    "\n",
    "    joblib.dump(modelo_final, ARQUIVO_MODELO)\n",
    "    with open(ARQUIVO_COLUNAS, 'w') as f:\n",
    "        json.dump(X.columns.tolist(), f)\n",
    "\n",
    "    print(f\"‚úÖ Modelo final treinado com sucesso!\")\n",
    "    print(f\"üíæ Modelo salvo em: '{ARQUIVO_MODELO}'\")\n",
    "    print(f\"üíæ Colunas salvas em: '{ARQUIVO_COLUNAS}'\")\n",
    "    print(\"\\n--- üöÄ PROJETO CONCLU√çDO ---\")\n",
    "\n",
    "# Executa o treinamento final com o melhor 'k' encontrado\n",
    "treinar_e_salvar_modelo_final(dados_limpos_ceara, melhor_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e4089",
   "metadata": {},
   "source": [
    "Depois de descobrir o melhor valor de `k`, estamos prontos para a etapa final.\n",
    "1.  **Treinamento**: N√≥s treinamos o modelo `KNeighborsRegressor` uma √∫ltima vez, usando o `k` otimizado e nosso conjunto de dados limpos.\n",
    "2.  **Salvamento**: Ao final, geramos dois arquivos cruciais:\n",
    "    - `modelo_knn.joblib`: Este √© o nosso **modelo treinado**, como se fosse o \"c√©rebro\" do nosso sistema, pronto para fazer novas previs√µes.\n",
    "    - `colunas_modelo.json`: Este √© o **\"manual de instru√ß√µes\"** do modelo. Ele guarda a lista exata de colunas (e a ordem delas) que o modelo precisa receber para funcionar corretamente.\n",
    "\n",
    "Com esses dois arquivos, podemos facilmente carregar nosso modelo em outra aplica√ß√£o (como um site ou um dashboard) para prever as notas de novos alunos sem precisar rodar todo o processo de limpeza e treinamento novamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e2da60",
   "metadata": {},
   "source": [
    "## üöÄ Conclus√£o  \n",
    "Este projeto transforma dados brutos em intelig√™ncia acion√°vel. Seguindo estes passos, constru√≠mos um sistema que aprendeu a encontrar padr√µes no perfil dos estudantes e, com base neles, fazer previs√µes sobre seu desempenho no ENEM. O uso do k-NN mostra como conceitos de \"similaridade\" e \"vizinhan√ßa\" podem ser ferramentas poderosas no mundo do Machine Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
