## üéØ Introdu√ß√£o e Motiva√ß√£o

Este projeto mergulha fundo nos dados do ENEM 2023 para **desvendar padr√µes de desempenho** dos estudantes. Meu objetivo principal foi criar um **dashboard interativo no Tableau** que permitisse explorar esses dados de diversas formas, tornando a an√°lise mais acess√≠vel e visual para qualquer um interessado nos resultados do exame.

Antes mesmo de colocar a m√£o no c√≥digo e iniciar o processo de ETL (Extract, Transform, Load), foi essencial um bom **planejamento e organiza√ß√£o**. A motiva√ß√£o por tr√°s deste projeto foi clara: responder a algumas perguntas-chave que nos dariam insights sobre o desempenho no ENEM.

### ‚ùì Quest√µes de Neg√≥cio a serem Respondidas pelo Dashboard:

Para guiar nossa an√°lise e garantir que o dashboard fosse √∫til, definimos as seguintes perguntas:

1.  **Desempenho por Localiza√ß√£o:** Qual o desempenho m√©dio dos alunos por Estado e Regi√£o?
2.  **Impacto Socioecon√¥mico:** Como o desempenho m√©dio dos alunos varia por faixa socioecon√¥mica?
3.  **Excel√™ncia na Reda√ß√£o:** Qual a distribui√ß√£o das maiores notas de reda√ß√£o por estado e munic√≠pio? Quantos alunos atingem essa nota m√°xima?
4.  **Distribui√ß√£o Geogr√°fica de Destaques:** Onde est√£o concentrados os alunos com as maiores notas por √°rea de conhecimento (Matem√°tica, Linguagens, etc.)?
5.  **G√™nero e Contexto Social:** Qual a distribui√ß√£o das notas por g√™nero e situa√ß√£o socioecon√¥mica?
6.  **Idade e Performance:** Como as notas se distribuem por faixa et√°ria?
7.  **Diversidade no Desempenho:** H√° diferen√ßas na distribui√ß√£o das notas por ra√ßa/cor?
8.  **Capital vs. Interior:** Quais s√£o as diferen√ßas nas m√©dias de notas entre alunos de capitais e do interior?
9.  **G√™nero por √Årea de Conhecimento:** Qual a compara√ß√£o das notas por g√™nero em cada √°rea de conhecimento?
10. **Ensino P√∫blico vs. Privado:** Qual a m√©dia de notas entre alunos de escolas p√∫blicas e privadas por √°rea de conhecimento?

Com essas perguntas em mente, conseguimos direcionar a aten√ß√£o para as colunas exatas necess√°rias no nosso conjunto de dados, otimizando todas as etapas de importa√ß√£o, limpeza e carregamento.

---

## üõ†Ô∏è Processo de ETL (Extract, Transform, Load)

Aqui detalhamos como os dados brutos foram transformados em um formato pronto para an√°lise no Tableau. Utilizamos **Python** com a biblioteca **Pandas** para realizar todas as etapas do ETL.

### **1. Extra√ß√£o - Carregando os Microdados do ENEM 2023**

A primeira etapa foi carregar os microdados do ENEM 2023. Este arquivo possui um volume consider√°vel de informa√ß√µes, ent√£o √© importante garantir que o caminho do arquivo esteja correto e que as op√ß√µes de leitura (como encoding e separador) estejam configuradas para evitar erros.

```python
import pandas as pd
import os

nome_do_arquivo = '../dados_brutos/MICRODADOS_ENEM_2023.csv'

# --- DEBUG ---
print(f"O Python est√° rodando a partir de: {os.getcwd()}")
print(f"Tentando carregar o caminho absoluto: '{nome_do_arquivo}'...")
# -------------

try:
    # Tenta carregar uma amostra para verificar se o arquivo est√° acess√≠vel e as configura√ß√µes est√£o corretas.
    # Usamos `nrows=100` para carregar apenas as 100 primeiras linhas, o que √© r√°pido para um teste.
    df_amostra = pd.read_csv(
        nome_do_arquivo, 
        encoding='latin-1', 
        sep=';', 
        nrows=100
    )

    # Se deu certo, mostra as 10 primeiras linhas e a lista completa de colunas encontradas.
    # Isso nos ajuda a confirmar a estrutura do arquivo.
    print("\n--- SUCESSO! ---")
    print("Amostra (10 primeiras linhas):")
    display(df_amostra.head(10)) 

    print("\n--- Lista completa de colunas encontradas: ---")
    print(df_amostra.columns.to_list())

except FileNotFoundError:
    print(f"\nERRO: Arquivo n√£o encontrado!")
    print(f"O caminho '{nome_do_arquivo}' ainda est√° incorreto.")
    print("Por favor, verifique se o nome do arquivo e das pastas est√£o id√™nticos, incluindo mai√∫sculas/min√∫sculas.")
except Exception as e:
    print(f"\nERRO ao tentar ler o arquivo: {e}")
```

<details>
<summary>‚ö° Clique para expandir a Sa√≠da do C√≥digo</summary>

```
caminho absoluto: 
'../dados_brutos/MICRODADOS_ENEM_2023.csv'...
--- SUCESSO! ---
Amostra (10 primeiras linhas):
	NU_INSCRICAO	NU_ANO	TP_FAIXA_ETARIA	TP_SEXO	TP_ESTADO_CIVIL	TP_COR_RACA	TP_NACIONALIDADE	TP_ST_CONCLUSAO	TP_ANO_CONCLUIU	TP_ESCOLA	...	Q016	Q017	Q018	Q019	Q020	Q021	Q022	Q023	Q024	Q025
0	210059085136	2023	14	M	2	1	1	1	17	1	...	C	C	B	B	A	B	B	A	A	B
1	210059527735	2023	12	M	2	1	0	1	16	1	...	B	A	B	B	A	A	C	A	D	B
2	210061103945	2023	6	F	1	1	1	1	0	1	...	B	A	A	B	A	A	A	A	A	B
3	210060214087	2023	2	F	1	3	1	2	0	2	...	A	A	A	B	A	A	D	A	A	B
4	210059980948	2023	3	F	1	3	1	2	0	2	...	A	A	A	B	A	A	B	A	A	A
5	210058061539	2023	6	F	1	3	1	1	0	1	...	B	A	A	B	A	A	C	A	A	B
6	210059855122	210059855122	11	F	1	3	1	1	12	1	...	B	A	A	B	A	B	B	A	A	B
7	210058387333	2023	11	M	1	3	1	1	12	1	...	B	A	A	A	A	A	B	A	B	B
8	210059085137	2023	5	F	1	2	1	1	1	1	...	B	A	A	B	A	A	C	A	A	B
9	210060801601	2023	11	M	1	1	1	1	8	1	...	B	A	B	C	B	A	C	A	B	B
10 rows √ó 76 columns
```

```
--- Lista completa de colunas encontradas: ---
['NU_INSCRICAO', 'NU_ANO', 'TP_FAIXA_ETARIA', 'TP_SEXO', 'TP_ESTADO_CIVIL', 'TP_COR_RACA', 'TP_NACIONALIDADE', 'TP_ST_CONCLUSAO', 'TP_ANO_CONCLUIU', 'TP_ESCOLA', 'TP_ENSINO', 'IN_TREINEIRO', 'CO_MUNICIPIO_ESC', 'NO_MUNICIPIO_ESC', 'CO_UF_ESC', 'SG_UF_ESC', 'TP_DEPENDENCIA_ADM_ESC', 'TP_LOCALIZACAO_ESC', 'TP_SIT_FUNC_ESC', 'CO_MUNICIPIO_PROVA', 'NO_MUNICIPIO_PROVA', 'CO_UF_PROVA', 'SG_UF_PROVA', 'TP_PRESENCA_CN', 'TP_PRESENCA_CH', 'TP_PRESENCA_LC', 'TP_PRESENCA_MT', 'CO_PROVA_CN', 'CO_PROVA_CH', 'CO_PROVA_LC', 'CO_PROVA_MT', 'NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT', 'TX_RESPOSTAS_CN', 'TX_RESPOSTAS_CH', 'TX_RESPOSTAS_LC', 'TX_RESPOSTAS_MT', 'TP_LINGUA', 'TX_GABARITO_CN', 'TX_GABARITO_CH', 'TX_GABARITO_LC', 'TX_GABARITO_MT', 'TP_STATUS_REDACAO', 'NU_NOTA_COMP1', 'NU_NOTA_COMP2', 'NU_NOTA_COMP3', 'NU_NOTA_COMP4', 'NU_NOTA_COMP5', 'NU_NOTA_REDACAO', 'Q001', 'Q002', 'Q003', 'Q004', 'Q005', 'Q006', 'Q007', 'Q008', 'Q009', 'Q010', 'Q011', 'Q012', 'Q013', 'Q014', 'Q015', 'Q016', 'Q017', 'Q018', 'Q019', 'Q020', 'Q021', 'Q022', 'Q023', 'Q024', 'Q025']
```

</details>

<br>

> üí° **Li√ß√£o Aprendida:** A etapa de planejamento √© fundamental\! Sem uma guia clara (as perguntas de neg√≥cio), n√£o saber√≠amos quais colunas realmente precisar√≠amos usar. Dessa forma, direcionamos a aten√ß√£o justamente para as colunas que respondem √†s quest√µes que motivam a cria√ß√£o do Dashboard, evitando carregar dados desnecess√°rios.

### **2. Carregamento Otimizado e Sele√ß√£o de Colunas Essenciais**

Ap√≥s a verifica√ß√£o inicial, carregamos o dataset completo, mas de forma otimizada. Para isso, selecionamos apenas as colunas que s√£o cruciais para responder √†s nossas perguntas de neg√≥cio. Isso reduz o consumo de mem√≥ria e acelera o processamento, algo vital para grandes volumes de dados como os do ENEM. √â importante notar que usamos `NO_MUNICIPIO_PROVA` e `SG_UF_PROVA` como proxy para a localiza√ß√£o de resid√™ncia, j√° que as colunas originais de resid√™ncia n√£o estavam dispon√≠veis.

```python
import time # Para cronometrar o tempo de carga

# Vamos usar a vari√°vel 'nome_do_arquivo' da C√©lula 1
# nome_do_arquivo = '../dados_brutos/MICRODADOS_ENEM_2023.csv' 

# --- 1. DEFINI√á√ÉO DAS COLUNAS (COM OS NOMES CORRIGIDOS) ---
# Selecionamos apenas as colunas que realmente precisamos para a an√°lise.
colunas_para_carregar = [
    'NU_INSCRICAO',
    'NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT', 'NU_NOTA_REDACAO',
    'TP_PRESENCA_CN', 'TP_PRESENCA_CH', 'TP_PRESENCA_LC', 'TP_PRESENCA_MT', 'TP_STATUS_REDACAO',
    
    # --- COLUNAS DE LOCALIZA√á√ÉO ---
    # Usando local da PROVA como substituto para local de RESID√äNCIA
    'NO_MUNICIPIO_PROVA', 
    'SG_UF_PROVA', 
    
    'TP_SEXO', 'TP_FAIXA_ETARIA', 'TP_COR_RACA',
    'TP_ESCOLA', 'Q006' # Q006 √© para renda familiar
]

print(f"Definidas {len(colunas_para_carregar)} colunas para carregar.")
print(f"Iniciando a carga do arquivo: '{nome_do_arquivo}'...")
print("(Isso pode levar alguns minutos...)")

# Medindo o tempo para acompanhar a performance da carga
start_time = time.time()

try:
    # --- 2. CARGA OTIMIZADA ---
    # O `usecols` garante que apenas as colunas especificadas sejam lidas, otimizando a mem√≥ria.
    df_enem_completo = pd.read_csv(
        nome_do_arquivo, 
        encoding='latin-1', 
        sep=';',
        usecols=colunas_para_carregar # Usando a lista corrigida de colunas
    )
    
    end_time = time.time()
    
    print(f"\n--- SUCESSO! ---")
    print(f"Arquivo completo carregado em {end_time - start_time:.2f} segundos.")
    print("DataFrame 'df_enem_completo' est√° pronto.")
    
    # --- 3. MOSTRAR OS DADOS ---
    print("\n--- Amostra (5 primeiras linhas dos dados carregados): ---")
    display(df_enem_completo.head())
    
    print("\n--- Informa√ß√µes (Tipos de dados e valores nulos): ---")
    df_enem_completo.info()

except Exception as e:
    print(f"\n--- ERRO DURANTE A CARGA ---")
    print(f"Erro: {e}")
    print("Se o erro persistir, verifique os nomes da lista 'colunas_para_carregar' novamente.")
```

<details>
<summary>‚ö° Clique para expandir a Sa√≠da do C√≥digo</summary>

```
Definidas 18 colunas para carregar.
Iniciando a carga do arquivo: 'C:/Users/Weillon Mota/Documents/Microdados ENEM/dados_brutos/MICRODADOS_ENEM_2023.csv'...
(Isso pode levar alguns minutos...)

--- SUCESSO! ---
Arquivo completo carregado em 23.64 segundos.
DataFrame 'df_enem_completo' est√° pronto.

--- Amostra (5 primeiras linhas dos dados carregados): ---
```

```
NU_INSCRICAO	TP_FAIXA_ETARIA	TP_SEXO	TP_COR_RACA	TP_ESCOLA	NO_MUNICIPIO_PROVA	SG_UF_PROVA	TP_PRESENCA_CN	TP_PRESENCA_CH	TP_PRESENCA_LC	TP_PRESENCA_MT	NU_NOTA_CN	NU_NOTA_CH	NU_NOTA_LC	NU_NOTA_MT	TP_STATUS_REDACAO	NU_NOTA_REDACAO	Q006
0	210059085136	14	M	1	1	Bras√≠lia	DF	0	0	0	0	NaN	NaN	NaN	NaN	NaN	NaN	F
1	2023	12	M	2	1	0	1	16	1	...	B	A	B	B	A	A	C	A	D	B
2	210061103945	6	F	1	1	Caxias do Sul	RS	1	1	1	1	502.0	498.9	475.6	363.2	1.0	700.0	C
3	210060214087	2	F	3	2	Fortaleza	CE	1	1	1	1	459.0	508.5	507.2	466.7	1.0	880.0	C
4	210059980948	3	F	3	2	Quixad√°	CE	1	1	1	1	402.5	379.2	446.9	338.3	1.0	560.0	B

```

```
--- Informa√ß√µes (Tipos de dados e valores nulos): ---
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3933955 entries, 0 to 3933954
Data columns (total 18 columns):
 #   Column              Dtype  
---  ------              -----  
 0   NU_INSCRICAO        int64  
 1   TP_FAIXA_ETARIA     int64  
 2   TP_SEXO             object 
 3   TP_COR_RACA         int64  
 4   TP_ESCOLA           int64  
 5   NO_MUNICIPIO_PROVA  object 
 6   SG_UF_PROVA         object 
 7   TP_PRESENCA_CN      int64  
 8   TP_PRESENCA_CH      int64  
 9   TP_PRESENCA_LC      int64  
 10  TP_PRESENCA_MT      int64  
 11  NU_NOTA_CN          float64
 12  NU_NOTA_CH          float64
 13  NU_NOTA_LC          float64
 14  NU_NOTA_MT          float64
 15  TP_STATUS_REDACAO   float64
 16  NU_NOTA_REDACAO     float64
 17  Q006                object 
dtypes: float64(6), int64(8), object(4)
memory usage: 540.2+ MB
```

</details>

### **3. Limpeza e Filtragem de Dados (Focando em Alunos V√°lidos)**

Esta etapa √© crucial para garantir que nossa an√°lise seja feita apenas com dados de alunos que realmente participaram do exame e tiveram suas reda√ß√µes avaliadas. Filtramos o DataFrame para incluir apenas os candidatos que estiveram presentes em **todas as quatro provas objetivas** e cuja **reda√ß√£o foi v√°lida** (status 1 = "Sem Problemas"). Isso remove `NaN` (valores nulos) das notas, garantindo a integridade dos c√°lculos.

```python
# --- C√âLULA 4: Limpeza e Transforma√ß√£o (Filtrando Presentes) ---

print("Iniciando a limpeza dos dados...")

# Verifica√ß√£o para garantir que o DataFrame principal foi carregado
if 'df_enem_completo' not in locals():
    print("ERRO: 'df_enem_completo' n√£o foi encontrado.")
    print("Por favor, rode a C√©lula de carga dos dados primeiro.")
else:
    print(f"Total de inscritos (linhas) no DataFrame original: {len(df_enem_completo)}")

    # 1. Filtrando quem estava PRESENTE em todas as provas
    #    (TP_PRESENCA == 1 significa 'Presente')
    filtro_presenca = (
        (df_enem_completo['TP_PRESENCA_CN'] == 1) &
        (df_enem_completo['TP_PRESENCA_CH'] == 1) &
        (df_enem_completo['TP_PRESENCA_LC'] == 1) &
        (df_enem_completo['TP_PRESENCA_MT'] == 1)
    )

    # 2. Filtrando quem teve a REDA√á√ÉO V√ÅLIDA (Status 1 = "Sem Problemas")
    #    Isso elimina notas zero por fuga ao tema, c√≥pia, etc., garantindo que s√≥ analisamos reda√ß√µes v√°lidas.
    filtro_redacao = (df_enem_completo['TP_STATUS_REDACAO'] == 1)

    # 3. Criando o novo DataFrame LIMPO
    #    Usamos .copy() para evitar avisos de SettingWithCopyWarning no futuro,
    #    criando uma c√≥pia independente do DataFrame filtrado.
    df_enem_presentes = df_enem_completo[filtro_presenca & filtro_redacao].copy()

    # 4. Verificando o resultado
    total_inscritos = len(df_enem_completo)
    total_presentes = len(df_enem_presentes)
    total_ausentes_ou_eliminados = total_inscritos - total_presentes
    
    print(f"\n--- Resumo da Limpeza ---")
    print(f"Total de alunos PRESENTES em todas as provas e com reda√ß√£o v√°lida: {total_presentes}")
    print(f"Total de alunos AUSENTES ou eliminados pelos filtros: {total_ausentes_ou_eliminados}")

    print("\n--- Amostra do DataFrame limpo (`df_enem_presentes.head()`): ---")
    # Agora, n√£o devem mais existir 'NaN' nas colunas de notas para esses alunos.
    display(df_enem_presentes.head())

    print("\n--- Verificando valores nulos nas notas (DEPOIS da limpeza): ---")
    # Confirmamos que as colunas de notas n√£o possuem mais valores nulos, como esperado.
    print(df_enem_presentes[['NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT', 'NU_NOTA_REDACAO']].isnull().sum())
```

<details>
<summary>‚ö° Clique para expandir a Sa√≠da do C√≥digo</summary>

```
Iniciando a limpeza dos dados...
Total de inscritos (linhas) no DataFrame original: 3933955

--- Resposta √† Pergunta 11 ---
Total de alunos PRESENTES e com reda√ß√£o v√°lida: 2585115
Total de alunos AUSENTES ou eliminados: 1348840

--- Amostra do DataFrame limpo (df_enem_presentes.head()): ---
```

```
	NU_INSCRICAO	TP_FAIXA_ETARIA	TP_SEXO	TP_COR_RACA	TP_ESCOLA	NO_MUNICIPIO_PROVA	SG_UF_PROVA	TP_PRESENCA_CN	TP_PRESENCA_CH	TP_PRESENCA_LC	TP_PRESENCA_MT	NU_NOTA_CN	NU_NOTA_CH	NU_NOTA_LC	NU_NOTA_MT	TP_STATUS_REDACAO	NU_NOTA_REDACAO	Q006
2	210061103945	6	F	1	1	Caxias do Sul	RS	1	1	1	1	502.0	498.9	475.6	363.2	1.0	700.0	C
3	210060214087	2	F	3	2	Fortaleza	CE	1	1	1	1	459.0	508.5	507.2	466.7	1.0	880.0	C
4	210059980948	3	F	3	2	Quixad√°	CE	1	1	1	1	402.5	379.2	446.9	338.3	1.0	560.0	B
9	210060801601	11	M	1	1	Batatais	SP	1	1	1	1	564.7	630.3	610.4	680.2	1.0	600.0	F
10	210059085130	8	M	3	1	Natal	RN	1	1	1	1	644.9	620.2	626.9	736.3	1.0	860.0	B

```

```
--- Verificando valores nulos nas notas (DEPOIS da limpeza): ---
NU_NOTA_CN           0
NU_NOTA_CH           0
NU_NOTA_LC           0
NU_NOTA_MT           0
NU_NOTA_REDACAO      0
dtype: int64
```

</details>

### **4. Enriquecimento Geogr√°fico - Regi√µes e Capitais**

Para responder √†s perguntas sobre desempenho por regi√£o e a diferen√ßa entre capitais e interior, enriquecemos o DataFrame com novas colunas. Mapeamos cada UF para sua respectiva Regi√£o (Norte, Nordeste, etc.) e classificamos os munic√≠pios como "Capital" ou "Interior".

```python
# --- C√âLULA 5: Transforma√ß√£o (Enriquecimento Geogr√°fico) ---

print("Iniciando a Etapa de Transforma√ß√£o (T)...")
print("Criando colunas 'NO_REGIAO' (para a Pergunta 1) e 'TP_CAPITAL' (para a Pergunta 8).")

# 1. Dicion√°rio de Mapeamento de Estados (UF) para Regi√µes
#    Usamos um dicion√°rio para "traduzir" as siglas dos estados para suas regi√µes correspondentes.
mapa_regioes = {
    'AC': 'Norte', 'AP': 'Norte', 'AM': 'Norte', 'PA': 'Norte', 'RO': 'Norte', 'RR': 'Norte', 'TO': 'Norte',
    'AL': 'Nordeste', 'BA': 'Nordeste', 'CE': 'Nordeste', 'MA': 'Nordeste', 'PB': 'Nordeste', 'PE': 'Nordeste', 'PI': 'Nordeste', 'RN': 'Nordeste', 'SE': 'Nordeste',
    'DF': 'Centro-Oeste', 'GO': 'Centro-Oeste', 'MT': 'Centro-Oeste', 'MS': 'Centro-Oeste',
    'ES': 'Sudeste', 'MG': 'Sudeste', 'RJ': 'Sudeste', 'SP': 'Sudeste',
    'PR': 'Sul', 'RS': 'Sul', 'SC': 'Sul'
}

# 2. Lista de Capitais Brasileiras
#    Esta lista √© usada para identificar se um munic√≠pio √© uma capital ou n√£o.
#    Aten√ß√£o aos acentos! Eles devem bater exatamente com os nomes dos munic√≠pios nos dados.
lista_capitais = [
    'Rio Branco', 'Macei√≥', 'Macap√°', 'Manaus', 'Salvador', 'Fortaleza', 'Bras√≠lia', 'Vit√≥ria', 'Goi√¢nia', 
    'S√£o Lu√≠s', 'Cuiab√°', 'Campo Grande', 'Belo Horizonte', 'Bel√©m', 'Jo√£o Pessoa', 'Curitiba', 'Recife', 
    'Teresina', 'Rio de Janeiro', 'Natal', 'Porto Alegre', 'Porto Velho', 'Boa Vista', 'Florian√≥polis', 
    'S√£o Paulo', 'Aracaju', 'Palmas'
]

try:
    # --- Criando a coluna NO_REGIAO (Pergunta 1) ---
    # O m√©todo `.map()` √© eficiente para aplicar o dicion√°rio de regi√µes √† coluna de UF.
    df_enem_presentes['NO_REGIAO'] = df_enem_presentes['SG_UF_PROVA'].map(mapa_regioes)
    
    # --- Criando a coluna TP_CAPITAL (Pergunta 8) ---
    # Usamos uma fun√ß√£o lambda com `.apply()` para verificar se cada munic√≠pio est√° na lista de capitais.
    df_enem_presentes['TP_CAPITAL'] = df_enem_presentes['NO_MUNICIPIO_PROVA'].apply(
        lambda municipio: 'Capital' if municipio in lista_capitais else 'Interior'
    )

    print("\n--- SUCESSO! Colunas 'NO_REGIAO' e 'TP_CAPITAL' criadas. ---")
    
    # --- Verifica√ß√£o ---
    print("\nAmostra dos dados com as novas colunas:")
    # Exibimos as colunas relevantes para confirmar que as novas colunas foram criadas corretamente.
    display(df_enem_presentes[['SG_UF_PROVA', 'NO_REGIAO', 'NO_MUNICIPIO_PROVA', 'TP_CAPITAL']].head())
    
    print("\nContagem de alunos por Regi√£o (verificando 'NO_REGIAO'):")
    print(df_enem_presentes['NO_REGIAO'].value_counts())
    
    print("\nContagem de alunos por Capital/Interior (verificando 'TP_CAPITAL'):")
    print(df_enem_presentes['TP_CAPITAL'].value_counts())

except Exception as e:
    print(f"\nERRO ao tentar criar as colunas de localiza√ß√£o: {e}")
    print("Verifique se o DataFrame 'df_enem_presentes' existe.")
```

<details>
<summary>‚ö° Clique para expandir a Sa√≠da do C√≥digo</summary>

```
Iniciando a Etapa de Transforma√ß√£o (T)...
Criando colunas 'NO_REGIAO' (Pergunta 1) e 'TP_CAPITAL' (Pergunta 8).

--- SUCESSO! Colunas 'NO_REGIAO' e 'TP_CAPITAL' criadas. ---

Amostra dos dados com as novas colunas:
```

```
	SG_UF_PROVA	NO_REGIAO	NO_MUNICIPIO_PROVA	TP_CAPITAL
2	RS	Sul	Caxias do Sul	Interior
3	CE	Nordeste	Fortaleza	Capital
4	CE	Nordeste	Quixad√°	Interior
9	SP	Sudeste	Batatais	Interior
10	RN	Nordeste	Natal	Capital
```

```
Contagem de alunos por Regi√£o (verificando 'NO_REGIAO'):
NO_REGIAO
Nordeste      948078
Sudeste       872372
Sul           281242
Norte         271389
Centro-Oeste  212034
Name: count, dtype: int64

Contagem de alunos por Capital/Interior (verificando 'TP_CAPITAL'):
TP_CAPITAL
Interior    1837010
Capital      748105
Name: count, dtype: int64
```

</details>

---

## üèóÔ∏è Modelagem de Dados - Star Schema

Para otimizar a performance das consultas no Tableau e organizar os dados de forma l√≥gica, aplicamos a modelagem de dados **Star Schema (Esquema em Estrela)**. Esta √© uma t√©cnica comum em Data Warehouses, onde temos uma tabela central de **Fatos** (com m√©tricas e chaves estrangeiras) e v√°rias tabelas de **Dimens√£o** (com atributos descritivos) conectadas a ela.

### **Vantagens do Star Schema:**

* **Simplicidade:** F√°cil de entender e navegar.
* **Performance:** Consultas mais r√°pidas, especialmente para ferramentas de BI como o Tableau.
* **Flexibilidade:** Permite adicionar novas dimens√µes e fatos facilmente.

Neste projeto, criamos as seguintes tabelas:
* **`fato_notas`**: A tabela central com as notas e chaves para as dimens√µes.
* **`dim_aluno`**: Cont√©m informa√ß√µes sobre os alunos.
* **`dim_localizacao`**: Cont√©m detalhes sobre a localiza√ß√£o (estado, munic√≠pio, regi√£o, capital/interior).

Vamos focar na cria√ß√£o da `fato_notas`, pois as `dim_aluno` e `dim_localizacao` seriam criadas a partir de transforma√ß√µes do `df_enem_presentes` (com a separa√ß√£o de atributos √∫nicos para cada dimens√£o e a gera√ß√£o de IDs substitutas).

![Modelagem Star Schema](modelagem_star_schema.png)

<details>
<summary>‚ö° Clique para expandir a Modelagem em Mermaid</summary>

```mermaid
graph LR
    %% --- Dimens√£o √† Esquerda ---
    DIM_ALUNO["**DIM_ALUNO**  
     
    üßç ID_Aluno (PK)  
    üî¢ NK_Inscricao (UNIQUE)  
    üéÇ TP_FAIXA_ETARIA  
    üöª TP_SEXO  
    üé® TP_COR_RACA  
    üè´ TP_ESCOLA  
    üí∞ Q006"]

    %% --- Tabela Fato no Centro ---
    FACT_NOTAS["**FATO_NOTAS**  
    üß© ID_Fato (PK)  
    üîë ID_Aluno (FK)  
    üîë ID_Local (FK)  
    üßÆ NU_NOTA_CN  
    üßÆ NU_NOTA_CH  
    üßÆ NU_NOTA_LC  
    üßÆ NU_NOTA_MT  
    üßÆ NU_NOTA_REDACAO"]

    %% --- Dimens√£o √† Direita ---
    DIM_LOCALIZACAO["**DIM_LOCALIZACAO**  
    
    üìç ID_Local (PK)  
    üèôÔ∏èNO_MUNICIPIO_PROVA
    üó∫Ô∏è SG_UF_PROVA  
    üåé NO_REGIAO  
    üè¢ TP_CAPITAL"]

    %% --- Liga√ß√µes (rela√ß√µes FK -> PK) ---
    DIM_ALUNO --> FACT_NOTAS
    FACT_NOTAS --> DIM_LOCALIZACAO

    %% --- Estilos ---
    style FACT_NOTAS fill:#BBDEFB,stroke:#1565C0,stroke-width:3px,color:#000,font-weight:bold
    style DIM_ALUNO fill:#C8E6C9,stroke:#2E7D32,stroke-width:2px,color:#000
    style DIM_LOCALIZACAO fill:#FFE082,stroke:#F9A825,stroke-width:2px,color:#000

    %% --- Layout geral e legenda ---
    subgraph LEGENDA[" "]
    direction LR
    DIM_ALUNO -. representa o aluno .-> FACT_NOTAS -. local da prova .-> DIM_LOCALIZACAO
    end
```

</details>



### **Cria√ß√£o da Tabela `fato_notas`**

A `fato_notas` √© o cora√ß√£o do nosso modelo. Ela agrupa as notas das diferentes provas (Ci√™ncias da Natureza, Ci√™ncias Humanas, Linguagens e C√≥digos, Matem√°tica e Reda√ß√£o) e as conecta √†s dimens√µes de `Aluno` e `Localiza√ß√£o` atrav√©s de chaves estrangeiras (IDs).

```python
# --- C√âLULA 9: Modelagem do Star Schema - fato_notas ---

print("Modelagem final: Criando a 'fato_notas' (Centro da Estrela)...")
print("(Este passo pode demorar alguns minutos, pois est√° juntando as tabelas)")

try:
    # 1. Come√ßamos com uma c√≥pia do nosso DataFrame limpo (`df_enem_presentes`),
    #    que j√° cont√©m as notas e as informa√ß√µes que se tornar√£o chaves naturais.
    fato_notas = df_enem_presentes.copy()

    # --- Juntando as Chaves Estrangeiras (FKs) ---

    # 2. Trazemos a Chave Estrangeira 'ID_Aluno' da 'dim_aluno'.
    #    Para isso, realizamos um 'merge' (jun√ß√£o) entre 'fato_notas' e 'dim_aluno'.
    #    A chave de jun√ß√£o √© 'NU_INSCRICAO' (no fato) e 'NK_Inscricao' (na dimens√£o),
    #    que s√£o as chaves naturais que identificam cada aluno.
    #    Um 'left' join garante que todas as linhas de 'fato_notas' sejam mantidas.
    fato_notas = pd.merge(
        fato_notas, 
        dim_aluno[['NK_Inscricao', 'ID_Aluno']],
        left_on='NU_INSCRICAO',
        right_on='NK_Inscricao',
        how='left' # 'left' join para garantir que n√£o perdemos nenhum aluno
    )

    # 3. Trazemos a Chave Estrangeira 'ID_Local' da 'dim_localizacao'.
    #    Aqui, a jun√ß√£o √© feita usando as colunas de localiza√ß√£o que j√° processamos,
    #    como 'NO_MUNICIPIO_PROVA', 'SG_UF_PROVA', 'NO_REGIAO', e 'TP_CAPITAL'.
    #    Isso vincula cada nota √† sua localiza√ß√£o geogr√°fica no nosso modelo dimensional.
    colunas_join_local = ['NO_MUNICIPIO_PROVA', 'SG_UF_PROVA', 'NO_REGIAO', 'TP_CAPITAL']
    fato_notas = pd.merge(
        fato_notas,
        dim_localizacao[['ID_Local'] + colunas_join_local],
        on=colunas_join_local,
        how='left' # 'left' join
    )

    print("Join das chaves estrangeiras conclu√≠do.")

    # --- Limpeza Final ---

    # 4. Selecionamos APENAS as colunas que s√£o Fatos (as notas) e as Chaves Estrangeiras (FKs).
    #    Isso garante que nossa tabela fato seja concisa e contenha apenas o que √© necess√°rio para as m√©tricas.
    colunas_fato = [
        'ID_Aluno',      # FK para dim_aluno
        'ID_Local',      # FK para dim_localizacao
        'NU_NOTA_CN',    # Fato: Nota de Ci√™ncias da Natureza
        'NU_NOTA_CH',    # Fato: Nota de Ci√™ncias Humanas
        'NU_NOTA_LC',    # Fato: Nota de Linguagens e C√≥digos
        'NU_NOTA_MT',    # Fato: Nota de Matem√°tica
        'NU_NOTA_REDACAO' # Fato: Nota de Reda√ß√£o
    ]
    fato_notas = fato_notas[colunas_fato]

    # 5. Criamos uma Chave Prim√°ria (PK) para a Tabela Fato.
    #    `ID_Fato` √© um identificador √∫nico para cada linha (cada conjunto de notas de um aluno em um local).
    fato_notas = fato_notas.reset_index(drop=True)
    fato_notas['ID_Fato'] = fato_notas.index + 1

    # 6. Reorganizamos as colunas para que a Chave Prim√°ria (PK) fique em primeiro.
    fato_notas = fato_notas[['ID_Fato', 'ID_Aluno', 'ID_Local', 'NU_NOTA_CN', 
                             'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT', 'NU_NOTA_REDACAO']]

    print(f"\n--- SUCESSO! 'fato_notas' criada. ---")
    print(f"Total de fatos (notas de alunos): {len(fato_notas)}")
    print("Amostra da Tabela Fato 'fato_notas':")
    display(fato_notas.head())

    print("\nInforma√ß√µes da 'fato_notas':")
    fato_notas.info()

except Exception as e:
    print(f"\n--- ERRO ao criar a fato_notas ---")
    print(f"Erro: {e}")
    print("Verifique se 'df_enem_presentes', 'dim_aluno' e 'dim_localizacao' existem e foram criadas corretamente.")
```

<details>
<summary>‚ö° Clique para expandir a Sa√≠da do C√≥digo</summary>

```
Modelagem final: Criando a 'fato_notas' (Centro da Estrela)...
(Este passo pode demorar alguns minutos, pois est√° juntando as tabelas)
Join das chaves estrangeiras conclu√≠do.

--- SUCESSO! 'fato_notas' criada. ---
Total de fatos (notas de alunos): 2585115
Amostra da Tabela Fato 'fato_notas':
```

```
ID_Fato	ID_Aluno	ID_Local	NU_NOTA_CN	NU_NOTA_CH	NU_NOTA_LC	NU_NOTA_MT	NU_NOTA_REDACAO
0	1	1	               1	    502.0	    498.9	475.6	        363.2	700.0
1	2	2	               2	    459.0	    508.5	507.2	        466.7	880.0
2	3	3	               3	    402.5	    379.2	446.9	        338.3	560.0
3	4	4	               4	    564.7	    630.3	610.4	        680.2	600.0
4	5	5	               5	    644.9	    620.2	626.9	        736.3	860.0

```

```
Informa√ß√µes da 'fato_notas':
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2585115 entries, 0 to 2585114
Data columns (total 8 columns):
 #   Column           Dtype  
---  ------           -----  
 0   ID_Fato          int64  
 1   ID_Aluno         int64  
 2   ID_Local         int64  
 3   NU_NOTA_CN       float64
 4   NU_NOTA_CH       float64
 5   NU_NOTA_LC       float64
 6   NU_NOTA_MT       float64
 7   NU_NOTA_REDACAO  float64
dtypes: float64(5), int64(3)
memory usage: 157.8 MB
```

</details>

### **5. Carregamento Final (Load) - Exportando o Data Warehouse**

Como etapa final do ETL, salvamos as tabelas do nosso Star Schema (`dim_localizacao`, `dim_aluno`, `fato_notas`) em arquivos CSV separados. Estes arquivos representam o nosso **Data Warehouse** e est√£o prontos para serem conectados a ferramentas de Business Intelligence como o Tableau. A exporta√ß√£o para CSV com `encoding='utf-8-sig'` garante que caracteres especiais (como acentos) sejam preservados corretamente, o que √© essencial para a qualidade dos dados no Tableau.

```python
import os # Usaremos para criar a pasta

# --- C√âLULA 10: Exporta√ß√£o do Data Warehouse (Load) ---

print("Iniciando a etapa 'Load' (Carga)...")
print("Exportando as tabelas do Star Schema para arquivos CSV...")

# 1. Definimos o nome da pasta onde nossos arquivos do Data Warehouse ser√£o salvos.
pasta_saida = 'data_warehouse'

# 2. Criamos a pasta de sa√≠da se ela ainda n√£o existir.
#    `exist_ok=True` evita um erro caso a pasta j√° exista.
os.makedirs(pasta_saida, exist_ok=True)
print(f"Salvando os arquivos na pasta: '{pasta_saida}'")

try:
    # 3. Salvamos cada uma das nossas tabelas (Dimens√µes e Fato) em arquivos CSV.
    #    `index=False` √© importante para n√£o salvar a coluna de √≠ndice do Pandas,
    #    que n√£o seria √∫til no Tableau e apenas ocuparia espa√ßo.
    #    `encoding='utf-8-sig'` √© uma escolha robusta para garantir que acentos e
    #    caracteres especiais sejam exibidos corretamente em diferentes softwares,
    #    incluindo o Excel e o Tableau.
    
    # --- Salvando dim_localizacao ---
    print("Salvando 'dim_localizacao.csv'...")
    path_local = os.path.join(pasta_saida, 'dim_localizacao.csv')
    dim_localizacao.to_csv(path_local, index=False, encoding='utf-8-sig')

    # --- Salvando dim_aluno ---
    print("Salvando 'dim_aluno.csv'... (Isso pode demorar um pouco)")
    path_aluno = os.path.join(pasta_saida, 'dim_aluno.csv')
    dim_aluno.to_csv(path_aluno, index=False, encoding='utf-8-sig')

    # --- Salvando fato_notas ---
    print("Salvando 'fato_notas.csv'... (Isso pode demorar um pouco)")
    path_fato = os.path.join(pasta_saida, 'fato_notas.csv')
    fato_notas.to_csv(path_fato, index=False, encoding='utf-8-sig')

    print("\n--- SUCESSO! Data Warehouse Exportado ---")
    print("O seu ETL e a modelagem Star Schema est√£o conclu√≠dos.")
    print("Voc√™ tem 3 arquivos na sua pasta 'data_warehouse':")
    print(f"1. {path_local}")
    print(f"2. {path_aluno}")
    print(f"3. {path_fato}")

except Exception as e:
    print(f"\n--- ERRO ao exportar os arquivos CSV ---")
    print(f"Erro: {e}")
    print("Verifique se voc√™ tem permiss√£o para escrever na pasta do projeto e se as tabelas existem.")
```

<details>
<summary>‚ö° Clique para expandir a Sa√≠da do C√≥digo</summary>

```
Iniciando a etapa 'Load' (Carga)...
Exportando as tabelas do Star Schema para arquivos CSV...
Salvando os arquivos na pasta: 'data_warehouse'
Salvando 'dim_localizacao.csv'...
Salvando 'dim_aluno.csv'... (Isso pode demorar um pouco)
Salvando 'fato_notas.csv'... (Isso pode demorar um pouco)

--- SUCESSO! Data Warehouse Exportado ---
O seu ETL e a modelagem Star Schema est√£o conclu√≠dos.
Voc√™ tem 3 arquivos na sua pasta 'data_warehouse':
1. data_warehouse\dim_localizacao.csv
2. data_warehouse\dim_aluno.csv
3. data_warehouse\fato_notas.csv
```

</details>

---

## üìä Visualiza√ß√£o de Dados com Tableau

Ap√≥s preparar e modelar os dados em um Star Schema, a fase seguinte foi lev√°-los para o Tableau, onde toda a m√°gica da visualiza√ß√£o acontece. Criamos um dashboard interativo que permite explorar as respostas para as perguntas de neg√≥cio definidas na introdu√ß√£o.

### **1. Conex√£o e Modelagem de Dados no Tableau**

* **Conex√£o das Fontes:** Conectamos o Tableau aos tr√™s arquivos CSV (`dim_aluno.csv`, `dim_localizacao.csv`, `fato_notas.csv`) que comp√µem nosso Data Warehouse.
* **Cria√ß√£o do Modelo Relacional:** No Tableau, recriamos visualmente o Star Schema, relacionando as tabelas de Dimens√£o (`dim_aluno`, `dim_localizacao`) com a tabela de Fatos (`fato_notas`).
    
    ![Tableau - Star Schema](tableau1.png)

    Isso garante que todas as nossas an√°lises e filtros funcionar√£o corretamente, puxando as informa√ß√µes das dimens√µes para contextualizar as m√©tricas de notas.

### **2. Cria√ß√£o de Par√¢metros e Filtros Din√¢micos**

Para oferecer uma experi√™ncia de usu√°rio interativa e responder √†s perguntas de forma flex√≠vel, implementamos:

* **Par√¢metros de Sele√ß√£o:** Criamos par√¢metros espec√≠ficos no Tableau para permitir que o usu√°rio selecione estados e munic√≠pios. Isso √© crucial para as Perguntas 3 e 4, e garante flexibilidade na explora√ß√£o geogr√°fica.
    * **Configura√ß√£o de Aliases:** Para uma melhor experi√™ncia de usu√°rio, configuramos aliases para siglas (ex: `AC` exibido como `Acre`) e c√≥digos para as categorias socioecon√¥micas (ex: `Q006` com as faixas de renda familiar).
* **Filtros Geogr√°ficos (Cascata):** Desenvolvemos filtros que se ajustam dinamicamente. Por exemplo, ao selecionar um estado, a lista de munic√≠pios dispon√≠veis no par√¢metro de munic√≠pio √© automaticamente atualizada para mostrar apenas os munic√≠pios daquele estado. Isso foi feito com campos calculados e conjuntos no Tableau, garantindo uma navega√ß√£o intuitiva.  

    ![Tableau - Star Schema](tableau2.png)  

    * **Uso de `UPPER(TRIM())`:** Mencionamos a import√¢ncia de padronizar textos nos c√°lculos de filtro para evitar incompatibilidades de mai√∫sculas/min√∫sculas ou espa√ßos extras.
    
* **Filtro de Faixa de Nota:** Para a distribui√ß√£o de notas, permitindo que o usu√°rio defina um intervalo de notas de interesse para destacar padr√µes no mapa e em outros gr√°ficos.

### **3. Desenvolvimento de KPIs e Gr√°ficos Chave**

Constru√≠mos diferentes visualiza√ß√µes para cada tipo de an√°lise, transformando os dados brutos em insights claros:

![Tableau - Star Schema](tableau3.png)  

![Tableau - Star Schema](tableau4.png)  

* **Mapa Interativo:** Para a distribui√ß√£o geogr√°fica de notas por munic√≠pio e estado, permitindo identificar √°reas de alto e baixo desempenho.
* **KPIs de Desempenho:**
    * **Maior Nota de Reda√ß√£o no Local:** Um indicador que mostra a nota m√°xima atingida em um estado/munic√≠pio selecionado, oferecendo um benchmark de excel√™ncia.
    * **Alunos com Nota M√°xima no Local:** Um contador que exibe quantos alunos alcan√ßaram essa nota m√°xima, dando uma perspectiva da dispers√£o da excel√™ncia. (Aqui o campo calculado `Alunos na Nota M√°xima por Estado` foi crucial, usando a fun√ß√£o `FIXED` para calcular a m√°xima por estado independentemente de outros filtros).
* **Gr√°ficos de Barras e Distribui√ß√£o:** Para analisar o desempenho m√©dio por estado/regi√£o, faixa socioecon√¥mica, g√™nero, ra√ßa, tipo de escola, etc., respondendo diretamente √†s perguntas de neg√≥cio.

### **4. Design e Layout do Dashboard**

Organizamos todas as planilhas e controles de filtro em um layout intuitivo e visualmente agrad√°vel. O design foi pensado para facilitar a navega√ß√£o, a interatividade e a compreens√£o r√°pida dos insights gerados, permitindo que o usu√°rio explore os dados sem dificuldades.

---

## ‚ú® Resultados e Insights

Explore de forma interativa os resultados do ENEM 2023 no meu dashboard!
L√° voc√™ pode visualizar o desempenho por cidade, comparar estados, analisar como a renda familiar e o tipo de escola influenciam as notas e descobrir padr√µes interessantes nos dados.

üëâ [Acesse o dashboard aqui](https://public.tableau.com/app/profile/weillon.mota/viz/enem_2023/DistribuioporCidade)  

---

## üöÄ Tecnologias Utilizadas

* **Python:** Linguagem de programa√ß√£o para o ETL.
* **Pandas:** Biblioteca Python para manipula√ß√£o e an√°lise de dados.
* **Tableau Public / Desktop:** Ferramenta de visualiza√ß√£o de dados e cria√ß√£o de dashboards.

---

## üë®‚Äçüíª Autor

**Weillon Mota**

* [Seu LinkedIn](https://www.linkedin.com/in/weillonmota/)  
* [Seu GitHub](https://github.com/weillonmota/projetos)  